---
title: "Objectives and Ideas"
author: "Katherine Goode"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    theme: cerulean
    highlight: textmate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This journal explains the objectives and ideas for the research project on applying LIME to the Hamby bullet data that is detailed in this series of journals. 

# Objectives

### Overall

- Understand how LIME works
- Consider ways to improvement LIME
- Apply LIME (or some improved version of LIME) to interpret the random forest model fit to the Hamby bullet data

### Ideas

**LIME diagnostics**  

- turn repetitive code into functions
- try out different weights
- change distance matrix in seriation to be 5-# of variables shared between two rows
- try using distance matrix computed on the feature value with seriation
- add in feature selection methods to LIME input options
- think of a way to compute consistency across top two features
- Siggi suggests refitting the RF model to the perturbations and then continuing with LIME with the RF predictions from the new model - this may help to understand if the problems are due to the sampling procedure or LIME itself
- he also suggested looking into SMOTE for dealing the imbalance in the classes with sampling
- compare the simple models based on different number of bins using an F-test
- look into computing a diversity or consistency measure for the sensitivity analysis
- include a penalty for the number of parameters when choosing bins
- look at the AUC after binning
- compute a likelihood ratio prob TRUE / prob FALSE from the LIME ridge regression
- try visualizing the features from the test data using dimension reduction and
coloring them by variables suggested to be important by lime
- Could try fitting a regression with interactions and see if LIME does a good job of explaining a model that is already interpretable.
- Maybe we could come up with a test to compare between global and local explanations (but how would we do this?)  
  
**Explainer Models**

- read through papers giving an overview of the current explanation methods
- look into iml (https://www.youtube.com/watch?v=jP6Rg13PEkQ)
- look more into Molnar and his adviser Bernd
- look into [MAPLE](https://blog.ml.cmu.edu/2019/07/13/towards-interpretable-tree-ensembles/)
    
**Understanding LIME**

- Run a simulation to understand if LIME is working
    - could implement a couple of local linear dependencies
    - piece this together
    - could include interactions in the model
    - does lime find the local models?
- edit details in understanding LIME journal
- look at the RF in the local region lime is considering to see if it agrees with the lime explanations
- try fitting LASSO logistic model and leave one out approach (for multicollinarity)
- assess lime results on logistic regression models
- try reticulate R package to apply python version of lime
- look into literature on binning methods
- think about why R^2 would be better for some binning methods
- read new paper on Anchor
    
**Possible Improvements to LIME**

- determine the best number of bins to use for each variable
- try out subsampling idea

**Simulation with RF**

- think about how to implement a simulation with a random forest
- look back at 601 notes from Kaiser for model assessment
- look at Hadley's 'removing the blindfold' for ideas
- read book Dr. Dixon lent me on sensitivity analyses

**Random Forests Confidence Intervals**

- look into papers by Giles Hooker and Lucas Mentch
- Dr. Nettleton paper

**Random Forest Interpretations**

- read https://www.r-bloggers.com/explaining-predictions-random-forest-post-hoc-analysis-randomforestexplainer-package/

### Possible Papers

Here are ideas for possible papers.

**Statistical Paper Presenting Visual Diagnostic Tools for LIME**

This paper would assess the explainations produced by LIME. It would introduce some visual diagnostics for LIME to assess its performance. Key points that will be made:

1. Explainer model is really bad (probably due to the binning)
2. Local explanations at not local

**Statistical Paper Assessing LIME**

This paper would be an explanation of how the application of LIME to the Hamby bullet data can help firearm examiners understand the random forest model predictions. When we first applied LIME to the bullet matching dataset, we found that the explanations were not as good as we wanted them to be. This has led us to try to understand why the LIME algorithm is not working well. We even tried some new sampling methods to improve the algorithm. They made a bit of improvement, but the improvements could be much better. This paper could discuss isusses we found with the current LIME algorithm, describe our proposed approaches, and compare the results.

**Firearm Examiner's Paper**

CSAFE is developing a predictive model that is able to match a bullet to the gun barrel that fired the bullet. They have acquired some sets of the data from the Hamby study. Part of this data was used to train a random forest model (`rtrees`), and the remaining portion of the data will be used as a testing dataset. CSAFE would now like to understand the predictions made by the random forest to be able to provide a better explanation of how the model works to the firearm examiners. Thus, it is of interest to apply the LIME algorithm to the predictions made by the random forest model on the testing data to see which variables LIME suggests are driving individual predictions.

# Concerns

The following are some of the concerns that we have with the current state of the LIME algorithm.

- I'm nervous about the fact that the results can change due to the permutations. Is there a way to check for consistency? Does this only happen if you have correlated variables, or can it also happen with uncorrelated variables?
- When you have a large number of predictions to assess, would it be a good idea to focus in on the ones that have the best fitting linear model or produce the most consistent results?
- What can be done to improve the linear regression model fit? Maybe adjusting the number of bins or the kernel width would help with this.
- We think that the model explainer needs to be close enough to the complex model that it is trying to explain in order to do a good job of providing explanations. For example, the binned regression works okay with neural networks, but it does not work well with a random forest. Maybe a tree explainer or a logistic regression explainer would work better with a random forest.

# General Thoughts

- LIME is kind of like a jackknife technique
