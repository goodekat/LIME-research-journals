---
title: "09-understanding_lime"
author: "Katherine Goode"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE, 
                      eval = TRUE)
```


# Overview 

This journal is a place where I create visuals to try to understand how lime works.

```{r}
library(cowplot)
library(furrr)
library(future)
library(glmnet)
library(gower)
library(lime)
library(randomForest)
library(tidyverse)

# Source functions
source("../../code/helper_functions.R")
```

# Generated Data

The images in this section were created using simulated data that I knew had a local trend around the point of interest. I consider both a linear regression and a ridge regression as the explainer model.

### Data and Model

This code generates the data and predictions from a hypothetical complex model.

```{r}
# Generate features and predictions from a hypothetical complex model
set.seed(20190624)
lime_data <- data.frame(feature1 = sort(runif(250, 0, 1)),
                        feature2 = sample(x = 1:250, 
                                          size = 250, 
                                          replace = FALSE)) %>%
  mutate(prediction = if_else(feature1 >= 0 & feature1 < 0.1, 
                              (0.3 * feature1) + rnorm(n(), 0, 0.01), 
                      if_else(feature1 >= 0.1 & feature1 < 0.3, 
                              rbeta(n(), 1, 0.5), 
                      if_else(feature1 >= 0.3 & feature1 < 0.5, 
                              sin(pi* feature1) + rnorm(n(), 0, 0.5),
                      if_else(feature1 >= 0.5 & feature1 < 0.8, 
                              -(sin(pi* feature1) + rnorm(n(), 0, 0.1)) + 1,
                      if_else(feature1 >= 0.8 & feature1 < 0.9, 
                              0.5 + runif(n(), -0.5, 0.5), 
                              0.5 + rnorm(n(), 0, 0.3)))))))
```

```{r}
# Specify a prediction of interest
prediction_of_interest <- data.frame(feature1 = 0.07, 
                                     feature2 = 200,
                                     prediction = 0.05, 
                                     color = factor("Prediction of Interest"))
```

```{r}
# Compute the distance between the prediction of interest and 
# all other observations
lime_data$distance <- (1 - gower_dist(x = prediction_of_interest, 
                                      y = lime_data))^20
```

### Linear Regression Explainer

These visualizations are based on a weighted linear regression model used as the explainer.

```{r}
# Fit the interpretable explainer model (weighted linear regression)
linear_explainer <- lm(formula = prediction ~ feature1 + feature2, 
                       data = lime_data, 
                       weights = distance)
```

```{r}
# Extract the coefficients from the explainer
linear_b0 <- coef(linear_explainer)[[1]]
linear_b1 <- coef(linear_explainer)[[2]]
linear_b2 <- coef(linear_explainer)[[3]]
```

```{r}
# Predictions versus feature 1
linear_plot_feature1 <- ggplot(data = lime_data,
                        mapping = aes(x = feature1, 
                                      y = prediction, 
                                      size = distance)) + 
  geom_point(color = "grey30",
             alpha = 0.75) + 
  geom_abline(intercept = linear_b0 +
                linear_b2*prediction_of_interest$feature2, 
              slope = linear_b1, 
              size = 1) +
  geom_point(data = prediction_of_interest,
             mapping = aes(x = feature1, y = prediction),
             fill = "#EFB084",
             color = "black",
             size = 5, 
             shape = 23) +
  theme_linedraw(base_family = "Times") +
  labs(x = "Feature 1", 
       y = "Black-Box Prediction") + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")
```

```{r}
# Predictions versus feature 1
linear_plot_feature2 <- ggplot(data = lime_data,
                        mapping = aes(x = feature2, 
                                      y = prediction, 
                                      size = distance)) + 
  geom_point(color = "grey30",
             alpha = 0.75) + 
  geom_abline(intercept = linear_b0 + 
                linear_b1*prediction_of_interest$feature1,
              slope = linear_b2,
              size = 1) +
  geom_point(data = prediction_of_interest,
             mapping = aes(x = feature2, y = prediction),
             fill = "#EFB084",
             color = "black",
             size = 5, 
             shape = 23) +
  theme_linedraw(base_family = "Times") +
  labs(x = "Feature 2", 
       y = "Black-Box Prediction") + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")
```

```{r}
# Plot that creates a size legend
size_legend_plot <- lime_data %>%
  gather(key = feature, value = value, 1:2) %>%
  ggplot(mapping = aes(x = value, 
                       y = prediction,
                       size = distance)) + 
  geom_point(color = "grey30",
             alpha = 0.75) +
  labs(size = "Weight") +
  theme_linedraw(base_family = "Times") +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

# Extract the legend
size_legend <- get_legend(size_legend_plot)
```

```{r}
# Join the two feature plots
lime_linear_plots <- plot_grid(linear_plot_feature1, 
                               linear_plot_feature2, 
                               size_legend,
                               ncol = 3, 
                               rel_widths = c(0.4, 0.4, 0.1))
```

```{r}
# Create a plot to extract a legend for the prediction of interest
lime_legend_plot <- ggplot(data = prediction_of_interest, 
                           mapping = aes(x = feature1, 
                                         y = prediction, 
                                         fill = color)) +
  geom_point(size = 5, shape = 23) +
  scale_fill_manual(values = c("#EFB084")) +
  theme_classic(base_family = "Times") +
  labs(fill = "") +
  theme(legend.position = "bottom")

# Extract the legend
lime_legend <- get_legend(lime_legend_plot)
```

```{r}
# Join the title, plots, legend, and caption into one figure
lime_linear_concept <- plot_grid(lime_linear_plots, lime_legend,
                         ncol = 1,
                         rel_heights = c(0.9, 0.1))
```

This example shows that the explainer model is doing a good job of capturing the local trend.

```{r}
# View the plot
lime_linear_concept
```

### Ridge Regression Explainer

```{r}
# Fits the explainer model (weighted ridge regression)
ridge_explainer <- glmnet(x = lime_data %>% 
                            select(feature1, feature2) %>%
                            as.matrix(), 
                          y = lime_data %>% pull(prediction),
                          weights = lime_data$distance,
                          family = "gaussian", 
                          alpha = 0, 
                          lambda = 1, 
                          standardize = FALSE)
```

```{r}
# Extract the coefficients from the explainer
ridge_b0 <- ridge_explainer$a0[[1]]
ridge_b1 <- ridge_explainer$beta[1,1]
ridge_b2 <- ridge_explainer$beta[2,1]
```

```{r}
# Predictions versus feature 1
ridge_plot_feature1 <- ggplot(data = lime_data,
                        mapping = aes(x = feature1, 
                                      y = prediction, 
                                      size = distance)) + 
  geom_point(color = "grey30",
             alpha = 0.75) + 
  geom_abline(intercept = ridge_b0 +
                ridge_b2*prediction_of_interest$feature2, 
              slope = ridge_b1, 
              size = 1) +
  geom_point(data = prediction_of_interest,
             mapping = aes(x = feature1, y = prediction),
             fill = "#EFB084",
             color = "black",
             size = 5, 
             shape = 23) +
  theme_linedraw(base_family = "Times") +
  labs(x = "Feature 1", 
       y = "Black-Box Prediction") + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")
```

```{r}
# Predictions versus feature 2
ridge_plot_feature2 <- ggplot(data = lime_data,
                        mapping = aes(x = feature2, 
                                      y = prediction, 
                                      size = distance)) + 
  geom_point(color = "grey30",
             alpha = 0.75) + 
  geom_abline(intercept = ridge_b0 + 
                ridge_b1*prediction_of_interest$feature1,
              slope = ridge_b2,
              size = 1) +
  geom_point(data = prediction_of_interest,
             mapping = aes(x = feature2, y = prediction),
             fill = "#EFB084",
             color = "black",
             size = 5, 
             shape = 23) +
  theme_linedraw(base_family = "Times") +
  labs(x = "Feature 2", 
       y = "Black-Box Prediction") + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")
```

```{r}
# Join the two feature plots
lime_ridge_plots <- plot_grid(ridge_plot_feature1, 
                              ridge_plot_feature2, 
                              size_legend,
                              ncol = 3,
                              rel_widths = c(0.4, 0.4, 0.1))
```

```{r}
# Join the title, plots, legend, and caption into one figure
lime_ridge_concept <- plot_grid(lime_ridge_plots, 
                                lime_legend,
                                ncol = 1,
                                rel_heights = c(0.9, 0.1))
```

This example shows that the explainer model is **not** doing a good job of capturing the local trend.

```{r}
# Preview the plot
lime_ridge_concept
```

# Bullet Data

### The Data

The cleaned training and testing datasets are loaded in below.

```{r}
# Load in the training data (Hamby Data 173 and 252)
hamby173and252_train <- read.csv("../../../data/hamby173and252_train.csv")

# Load in the testing data (Hamby Data 224 Sets 1 and 11)
hamby224_test <- read.csv("../../../data/hamby224_test.csv")
```

A vector containing the features used in the random forest `rtrees` is created below that will be used when creating some of the plots in this journal.

```{r}
# Obtain features used when fitting the rtrees random forest
rf_features_ordered <- data.frame(feature = rownames(bulletr::rtrees$importance), 
                                  MeanDecreaseGini = bulletr::rtrees$importance) %>%
  arrange(desc(MeanDecreaseGini))

top2_features <- rf_features_ordered$feature[1:2]
```

### Random Forest Model

```{r}
# Fit a random forest model with the top two most important features in the model
if(!file.exists("./data/rfmodel.rds")) {
  
  # Fit the random forest  
  rfmodel <- randomForest(factor(samesource) ~ rough_cor + ccf, data = hamby173and252_train)

  # Save the random forest
  saveRDS(rfmodel, "./data/rfmodel.rds")
  
} else {
  
  # Load the rf model
  rfmodel <- readRDS("./data/rfmodel.rds")
}
```

```{r}
# Create a dataset with the test data input to the random forest model 
# and the predictions from the random forest model
hamby224_test_subset <- hamby224_test %>%
  na.omit() %>%
  select(case:land2, rough_cor, ccf, samesource) %>%
  mutate(rfpred = predict(rfmodel, 
                           newdata = hamby224_test %>% select(rough_cor, ccf) %>% na.omit()),
         rfscore = predict(rfmodel, 
                           newdata = hamby224_test %>% select(rough_cor, ccf) %>% na.omit(),
                           type = "prob")[,2])
```

### Applying LIME

The functions `lime` and `explain` from the lime package are applied below to the random forest model.

```{r eval = FALSE}
# Create or load the lime and explain objects for the main effects logisitic regression model
if(!file.exists("./data/lime_explain_sub.rds")) {
  
  # Set a seed
  set.seed(20190315)
  
  # Apply lime
  lime_sub <- lime(x = hamby173and252_train %>% select(rough_cor, ccf), 
                   model = as_classifier(rfmodel))
  
  # Apply explain
  explain_sub <- explain(x = hamby224_test %>%
                             select(rough_cor, ccf) %>% 
                             na.omit(), 
                           explainer = lime_sub, 
                           n_labels = 1,
                           n_features = 2)
  
  # Join the lime and explain objects in a list
  lime_explain_sub <- list(lime = lime_sub, explain = explain_sub)
  
  # Save the lime and explain objects
  saveRDS(lime_explain_sub, "./data/lime_explain_sub.rds")
  
} else {
  
  # Load the lime and explain objects
  lime_explain_sub <- readRDS("./data/lime_explain_sub.rds")
 
}
```

```{r eval = FALSE}
create_bin_df <- function(bin_cut) {
 data.frame(quantile = c("0%", "25%", "50%", "75%", "100%"), 
            bin_cut) 
}

bin_cuts <- purrr::map_df(lime_explain_sub$lime$bin_cuts, 
                          create_bin_df,
                          .id = "feature") %>%
  spread(key = feature, value = bin_cut)

bin_cuts
```

### Visualizations of Data, Model, and LIME

```{r eval = FALSE}
(case1 <- lime_explain_sub$explain %>% filter(case == 1))
```

```{r eval = FALSE}
ggplot() +
  geom_point(data = hamby173and252_train %>% filter(samesource == FALSE),
             aes(x = rough_cor, y = ccf),
             color = "darkgrey",
             alpha = 0.2) +
  geom_point(data = hamby173and252_train %>% filter(samesource == TRUE),
             aes(x = rough_cor, y = ccf),
             color = "darkorange") + 
  geom_vline(xintercept = bin_cuts$rough_cor) + 
  geom_hline(yintercept = bin_cuts$ccf) +
  theme_minimal()
```


```{r eval = FALSE}
ggplot() + 
  geom_point(data = hamby224_test_subset, 
             aes(x = rough_cor, y = ccf, color = rfscore),
             alpha = 0.5) + 
  scale_color_gradient2(low = "darkgrey", high = "darkorange", midpoint = 0.5) +
  geom_point(data = lime_explain_sub$explain %>% 
               filter(case == 1) %>%
               select(feature, feature_value) %>%
               spread(key = feature, value = feature_value),
             aes(x = rough_cor, y = ccf)) +
  geom_vline(xintercept = bin_cuts$rough_cor) + 
  geom_hline(yintercept = bin_cuts$ccf) + 
  theme_minimal()
```

# Session Info

```{r}
sessionInfo()
```

