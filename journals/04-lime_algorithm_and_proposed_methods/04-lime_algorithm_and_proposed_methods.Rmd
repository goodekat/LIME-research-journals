---
title: "04-lime_algorithm_and_proposed_methods"
author: "Katherine Goode"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE)
```

# Overview

This journal contains a draft of the LIME algorithm and ideas for different binning methods that may be better than the ones currently implemented in the LIME package.

```{r}
# Load libraries

# Source functions
source("../../code/helper_functions.R")
```

# The Data

The cleaning training and testing datasets are loaded in below.

```{r}
# Load in the training data (Hamby Data 173 and 252)
hamby173and252_train <- read.csv("../../data/hamby173and252_train.csv")
```

# Draft of LIME Procedure

I wrote this up to start thinking about how to describe the procedure that the `lime` R package uses to implement the LIME algorithm. It needs a lot of work, but it is a start. The final version of this will end up in the technical stats paper critiquing LIME. The version in the firearm examiner's paper will be much simpler...

The steps below explain the procedure that the R package is using to apply the LIME algorithm to the bullet matching predictions on the Hamby 224 clone dataset made by the random forest model from Hare. For simplicity, the steps are described as what happens to one case in the test data. Thus, the steps (2) through (7) are repeated for each observation in the testing dataset.

Let
  $$Y_{jk} = \begin{cases} 
  1 & \mbox{ if bullets } j \mbox{ and } k \mbox{ were fired from the same gun barrel }\\
  0 & \mbox{otherwise}
  \end{cases}$$
be the response variable in the training dataset, and $X_1,...,X_9$ correspond to the nine features in the training dataset. Let $X'_1,...,X'_9$ be the 

1. Distributions for each of the features in the training data are obtained. 
    + The method that `lime` uses to obtain the distribution differs based on the feature type. All of the features in the Hamby datasets are numeric. For numeric features, the default option in `lime` (`quantile_bins = TRUE`) computes the quantiles of each feature based on the number of bins selected. The default number of bins is 4 (`n_bins = 4`).

2. Many ($n$) samples from each of the feature distributions are drawn.
     + To do this, `lime` has several options (mostly quoted from `lime` package for now):
      * bin\_continuous = TRUE should continuous variables be binned?
      * quantile\_bins = TRUE should the ins for n\_bins be based on quantiles or spread evenly
      * n\_bins = 4 number of bins if bin\_continuous is TRUE
      * use\_density = TRUE if bin\_continuous is FALSE, should continuous data be sampled using kernel density estimation (if not, then will assume normal for continuous variable)

3. Predictions for the testing data using the random forest model are computed.
     + The random forest model `rtrees` is used to make a prediction for the observation from the test dataset and each of the $n=5000$ samples as to whether or not the comparison of the two bullets in the test case are a match. Since the random forest is a classification model, `lime` is set to return the prediction probabilities.

4. Similarity score between the observation in the testing data and each of the $n=5000$ sampled values are obtained.
    + The way that the similarity score is computed depends on the type of feature. Since all of the features in the Hamby 224 test dataset are continuous, the simulated values are first converted into 0-1 features where a 1 indicates that the feature from the simulated value falls in the same bin as the observed data point and a 0 indicates that the feature is not in the same bin as the observed data point. Then, by default, the Gower distance is used to compute the similarity score. (using the `gower` package in R)

5. Feature selection is performed by fitting some type of regression model weighted by the similarity scores is to the simulated data and the observed value. The 0-1 versions of the features are used.
    + The user can specify the number of features, $m$, they would like to select to explain the prediction. `lime` supports the following options for feature selection
      * forward selection with ridge regression
      * highest weight with ridge regression
      * LASSO model
      * tree model
      * default: forward selection if $m\le6$ with a ridge regression model, highest weight with ridge regression otherwise

6. A ridge regression model is fit as the simple model by regressing the prediction probabilities on the $m$ selected predictor variables and weighted by the similarity scores. If the response is categorical, the user can select how many categories and which categories they want to explain.
  $$P(Match = TRUE) = \beta_0 + 
  \beta_1 \cdot I\left[X_1 \in \mbox{obs bin}\right] + 
  \beta_2 \cdot I\left[X_2 \in \mbox{obs bin}\right] + 
  \beta_3 \cdot I\left[X_3 \in \mbox{obs bin}\right]$$
For the prediction of interest, 
  $$P(Match = TRUE) = \beta_0 + \beta_1 + \beta_2 + \beta_3.$$
7. The feature weights are extracted and used as the explanations.

Note: I realized that if `bin_continuous = FALSE`, then bins are not used at all. Instead, a kernel density estimator is used to sample from the distribution (or a normal distribution if specified), and then the ridge regression models are fit without "numerified" values.


# Inputs for LIME Functions

The default input settings for the `lime` and `explain` functions in the lime package are described below.

### `lime` function 

The default settings for the `lime` function are as follows.

- `bin_continuous = TRUE`: If set to `TRUE`, the continuous variables will be binned when making the explanations. If they are not binned, then perturbations will be obtained by either simulating using kernel density estimation or a normal distribution depending on what the option of `use_density` is set to. 
- `bins = 4`: The number of bins to divide the continuous variables into. The default is 4 bins.
- `quantile_bins = TRUE`: If set to `TRUE`, the bins will be be based on `n_bins` quantiles. Otherwise, the bins will be spread evenly over the range of the training data.
- `use_density = TRUE`: If `bin_continuous` is set to `FALSE` and this is set to `TRUE`, then the continuous data will be sampled using kernel density estimation. Otherwise, it will be assumed that the continuous features follow a normal distribution and samples will be drawn from a normal distribution with the mean and standard deviation set to the sample mean and standard deviation associated with the feature.

### `explain` function 

For our research project, the number of labels (`n_labels`) for the response variable will be set to 1 and the number of features to include in the explanations (`n_features`) will be set to 2. The default values for the other options are as follows. 

- `n_permutations = 5000`: The number of perturbations generated for each feature.
- `feature_select = 'auto'`: This is the feature selection method for choosing the number of features specified. This uses forward selection if $m\le 6$ and otherwise highest weights. The other feature selection options are as follows.
    - `none`: Use all features for the explanation. Not advised unless you have very few features.
    - `forward selection`: Features are added one by one based on their improvements to a ridge regression fit of the complex model outcome.
    - `highest weights`: The m features with highest absolute weight in a ridge regression fit of the complex model outcome are chosen.
    - `lasso`: The m features that are least prone to shrinkage based on the regularization path of a lasso fit of the complex model outcome is chosen.
    - `tree`: A tree is fitted with log2(m) splits, to use at max m features. It may possibly select less.
- `dist_fun = 'gower'`: The distance function to be used for determining how close a perturbation is to the test point.
- `kernel_width = NULL`: If `dist_fun` is not set to `gower`, then this is the kernel width that will be used in the distance function.

# Proposed Bin Creation Methods

### Subsampled Bins

One idea that I had to try to improve the lime explanations was to subsample from the training dataset before computing the quantiles. The proportion of observations with `samesource = FALSE` is very large in the training dataset, so that overwhelms the observations with `samesource = TRUE`. By subsampling, it might allow for better locations to bin the data. I wrote the code below to attempt to apply lime to a subsampled version of the training dataset.

```{r eval = FALSE}
# Code saved for future use for the sampling data

# Create the data (with a set seed)
set.seed(20181128)
hamby173and252_train_sub <- hamby173and252_train %>%
  filter(samesource == FALSE) %>%
  slice(sample(1:length(hamby173and252_train$barrel1), 4500, replace = FALSE)) %>%
  bind_rows(hamby173and252_train %>% filter(samesource == TRUE))

# Save the subsampled data
write.csv(hamby173and252_train_sub, "../../data/hamby173and252_train_sub.csv", row.names = FALSE)

# Apply lime to the subsampled training data with the specified input options
hamby224_lime_explain_sub <- future_pmap(.l = as.list(hamby224_lime_inputs %>%
                                                        select(-case)),
                                         .f = run_lime, # run_lime is one of my helper functions
                                         train = hamby173and252_train_sub %>% select(rf_features),
                                         test = hamby224_test %>% arrange(case) %>% select(rf_features) %>% na.omit(),
                                         rfmodel = as_classifier(rtrees),
                                         label = "TRUE",
                                         nfeatures = 3,
                                         seed = TRUE) %>%
  mutate(training_data = factor("Subsampled"))

# Separate the lime and explain function results from the subsampled data
hamby224_lime_sub <- map(hamby224_lime_explain_sub, function(list) list$lime)
hamby224_explain_sub <- map_df(hamby224_lime_explain_sub, function(list) list$explain)

# Join the lime results from the full and subsampled training data
hamby224_lime <- c(hamby224_lime_full, hamby224_lime_sub)
hamby224_explain <- bind_rows(hamby224_explain_full, hamby224_explain_sub)
```

### Tree Based Bins

Heike suggested the idea of using classification trees to choose the cutoffs for the lime bins. That is, the divisions in the tree could be used as the bin cuts. This would allow us to automate the process and to obtain a penalty parameter since the trees give us nesting. (MSE + lambda * p where p is the from the number of trees multiplied by the number of ...)

Heike wrote the function `treebink` to take x and y variables for fitting a tree and returning the cuts for $k$ bins based on the splits from the tree. The function is stored in the file   `helper_functions.R`. Here is an example using the function below with the predictor variable of ccf.

```{r}
# Example using treebink
treebink(y = hamby173and252_train$samesource, x = hamby173and252_train$ccf, k = 5)
```

However, there seems to be a problem with `treebink`. For certain variables, when a high number of bins is requested, the function returns more than the desired number of bins. Heike is going to look into this.

She recently updated the function `treebink` to run using the tree package. Here are some comments from her about the code: 

"I've moved it from rpart to tree. I am not quite sure that the two packages are doing the exact same thing. They claim they do but they might be implemented a bit differently. What we are doing now is if there are too many splits, we look at the order in which the splits are made and roll back some of them. I haven't found a case that doesn't work, but obviously that doesn't show that there isn't any :)"

I wrote the function `mylime`, which adds an option in the `lime` function from the LIME package to use Heike's function `treebink` to obtain tree based bins. This function is also included in the file `helper_functions.R`. The output from `mylime` can then be input into the `explain` function to obtain explanations. I was able to rerun all of the code for creating the `hamby224_test_explain` dataset with the explanations from the tree based bins included for 2 to 6 bins.

# Session Info

```{r}
sessionInfo()
```