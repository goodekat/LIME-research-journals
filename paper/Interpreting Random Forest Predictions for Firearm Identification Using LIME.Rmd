---
title: "Interpreting Random Forest Predictions for Firearm Identification Using LIME"
author: "Katherine Goode and Heike Hofmann"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: true
bibliography: references.bib
---

```{r setup, include = FALSE}
# Set knitr options
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, fig.pos = "center")

# Load libraries
library(tidyverse)
library(bulletr)
library(randomForest)
library(lime)
library(cowplot)
library(purrr)

# Source functions
source("../code/helper_functions.R")
```

# Introduction

(need to finish adding sources)

The discipline of firearm identification examines bullets to determine the likelihood that a bullet found in a criminal case was fired from a particular gun. To do this, the bullet from the crime will be compared with a bullet that was known to be fired from the gun under evaluation. Traditionally, this is a procedure that has been performed by hand. Specially trained examiners visually compare the microscopic bullet striations that were created when the bullets passed through the gun barrel. Often a comparison microscope is used that allows the examiners to view both bullets at the same time (National Research Council 2009). The examiners use this ability to determine whether the two bullets were fired from the same gun barrel.

Recently, the scientific community has been encouraging the inclusion of more data driven techniques to be used in forensic investigations. These methods would allow for the reporting of a measure of uncertainty in addition to the conclusion drawn from the analysis. This led Hare, Hofmann, and Carriquiry (2017) to propose a new computer automated method of bullet matching that could supplement the visual inspection by the firearm examiners. Their method involves obtaining bullet signatures of the striations from the scans of the two bullets, computing variables that measure the similarity of the two signatures, and using a trained random forest model to determine the probability of a match based on the similarity variables. They trained their model on a set of known bullet matches and non-matches from the Hamby study. They demonstrated how the random forest model can be used to make prediction on a new set of bullet signature comparisons. The results from the paper suggest that the random forest model leads to highly accurate bullet matching predictions. The authors found that when their model was applied to a testing dataset (?), the resulting error rate was 0%.

While random forest models often result in good predictions as seen in Hare, Hofmann, and Carriquiry, it is well known that a disadvantage of random forest models and other machine learning techniques is that it is difficult to interpret the models (reference?). For example, it is not possible to tell which variables played an important role in the creation of individual predictions. This issue led to the development of LIME (reference), which is an algorithm that examines the behavior of the complicated model on a local scale around a new prediction using a linear regression model. This allows for the ability to understand which were the driving variables that led to a prediction of interest.

Since firearm identification identification is commonly used as evidence for convictions in court cases, it is important to be able to understand and assess the model that is being used to quantify the probability that a bullet was fired from a gun. LIME provides the ability to understand which were the key variables used by the random forest model to make a prediction, which would allow firearm examiners to check whether or not the predictions created by the random forest are based on reasonable variables. 

This paper provides an example of the application of LIME to a bullet matching problem. (provide more details about what is contained in the papers - i.e. section 2 describes the Hamby data; section 3 describes the random forest model and LIME; ...)

# Data

## Training Data: The Hamby 173 and 252 Datasets

```{r}
# Load in the training data (Hamby Data 173 and 252)
hamby173and252_train <- read.csv("../data/hamby173and252_train.csv")
```

```{r}
# Obtain features used when fitting the rtrees random forest
rf_features <- rownames(rtrees$importance)
```

```{r fig.height = 5, fig.width = 8}
# Create plots of the feature distributions colored by samesource for the training data
hamby173and252_train %>% 
  select(rf_features, samesource) %>%
  gather(key = feature, value = value, 1:9) %>%
  select(feature, value, samesource) %>%
  ggplot(aes(x = value, fill = samesource)) + 
  geom_histogram(bins = 30) + 
  facet_wrap( ~ feature, scales = "free") +
  labs(x = "Variable Value", y = "Frequency", fill = "Same Source?") +
  theme_bw() +
  theme(legend.position = "bottom")
```

```{r fig.height = 5, fig.width = 8}
# Create plots of the feature distributions colored by samesource for the training data
# using the position = "fill" option
hamby173and252_train %>% 
  select(rf_features, samesource) %>%
  gather(key = feature, value = value, 1:9) %>%
  select(feature, value, samesource) %>%
  ggplot(aes(x = value, fill = samesource)) + 
  geom_histogram(position = "fill", bins = 30) + 
  facet_wrap( ~ feature, scales = "free") +
  labs(x = "Variable Value", y = "Proportion", fill = "Same Source?") +
  theme_bw() +
  theme(legend.position = "bottom")
```

```{r fig.height = 4, fig.width = 9}
# Create a correlation heatmap of the match comparisons in the training data
cor_match <- hamby173and252_train %>%
  select(rf_features, samesource) %>%
  filter(samesource == TRUE) %>%
  select(-samesource) %>%
  cor() %>%
  reshape2::melt() %>%
  mutate(Var1 = factor(Var1, levels = c("ccf", "cms", "matches", "rough_cor", "sum_peaks",
                                        "D", "sd_D", "mismatches", "non_cms")),
         Var2 = factor(Var2, levels = c("ccf", "cms", "matches", "rough_cor", "sum_peaks",
                                        "D", "sd_D", "mismatches", "non_cms"))) %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() + 
  scale_fill_gradient2(limits = c(-1, 1)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") + 
  labs(x = "", y = "", fill = "Correlation",
       title = "Match Comparisons")

# Create a correlation heatmap of the non-match comparisons in the training data
cor_nonmatch <- hamby173and252_train %>%
  select(rf_features, samesource) %>%
  filter(samesource == FALSE) %>%
  select(-samesource) %>%
  cor() %>%
  reshape2::melt() %>%
  mutate(Var1 = factor(Var1, levels = c("ccf", "cms", "matches", "rough_cor", "sum_peaks",
                                        "D", "sd_D", "mismatches", "non_cms")),
         Var2 = factor(Var2, levels = c("ccf", "cms", "matches", "rough_cor", "sum_peaks",
                                        "D", "sd_D", "mismatches", "non_cms"))) %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() + 
  scale_fill_gradient2(limits = c(-1, 1)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") +
  labs(x = "", y = "", fill = "Correlation",
       title = "Non-Match Comparisons")

# Create a title for the panel of plots
cor_title <- ggdraw() +
  draw_label("Correlation of Feature Variables in the Training Data", 
             fontface = "bold", 
             size = 16,
             x = 0.5,
             hjust = 0.5)

# Create a joined legend for the panel of plots
cor_legend <- get_legend(cor_match + theme(legend.position = "right"))

# Create the panel of plots
plot_grid(cor_title, 
          plot_grid(cor_match, cor_nonmatch, cor_legend, ncol = 3, rel_widths = c(2, 2, 0.5)),
          ncol = 1,
          rel_heights = c(0.25, 3))
```

## Testing Data: The Hamby 224 Clone

The Hamby 224 Clone is organized as a test set of a cloned (sub-)set of the Hamby 224 bullets. As with all Hamby sets [@hamby], Hamby set 224, is a collection of 35 bullets, organized as 20 known bullets and 15 questioned bullets. The known bullets are fired in pairs of two through one of ten consecutively manufactures P-85 barrels.
Clone set 224 is arranged as a test set of fifteen tests, one for each questioned bullet. Each test set is arranged as a combination of three bullets: two known bullets and a questioned bullet. The test asks for a decision on whether the questioned bullet comes from the same source as the two known bullets  or from a different source. This situation is similar to what a firearms and toolmarks examiner might encounter in case work. 

```{r}
# Load in the Hamby 224 datasets
hamby224_set1 <- readRDS("../data/h224-set1-features.rds")
hamby224_set11 <- readRDS("../data/h224-set11-features.rds")
```

```{r}
# If the test data does not exist, clean the data, create the test data, and save it
# Otherwise, load in the test data
if(!file.exists("../data/hamby224_test.csv")) {
  
  # Clean the Hamby 224 set 1 data
  hamby224_set1_cleaned <- hamby224_set1 %>%
    select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
    rename(bullet1 = bulletA,
           bullet2 = bulletB, 
           land1 = landA,
           land2 = landB) %>%
    mutate(study = factor("Hamby 224"), 
           set = factor("Set 1"),
           bullet1 = recode(factor(bullet1), 
                            "1" = "Known 1", "2" = "Known 2", "Q" = "Questioned"),
           bullet2 = recode(factor(bullet2), 
                            "1" = "Known 1", "2" = "Known 2", "Q" = "Questioned"),
           land1 = recode(factor(land1), 
                          "1" = "Land 1", "2" = "Land 2", "3" = "Land 3", 
                          "4" = "Land 4", "5" = "Land 5", "6" = "Land 6"),
           land2 = recode(factor(land2), 
                          "1" = "Land 1", "2" = "Land 2", "3" = "Land 3", 
                          "4" = "Land 4", "5" = "Land 5", "6" = "Land 6")) %>%
    select(study, set, bullet1:land2, rf_features, rfscore, samesource)

  # Clean the Hamby 224 set 11 data
  hamby224_set11_cleaned <- hamby224_set11 %>%
    select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
    rename(bullet1 = bulletA,
           bullet2 = bulletB, 
           land1 = landA,
           land2 = landB) %>%
    mutate(study = factor("Hamby 224"), 
           set = factor("Set 11"),
           bullet1 = recode(factor(bullet1), 
                            "Bullet 1" = "Known 1", "Bullet 2" = "Known 2", 
                            "Bullet I" = "Questioned"),
           bullet2 = recode(factor(bullet2), 
                            "Bullet 1" = "Known 1", "Bullet 2" = "Known 2", 
                            "Bullet I" = "Questioned")) %>%
    select(study, set, bullet1:land2, rf_features, rfscore, samesource)

  # Create a dataset with all combinations of lands and bullets comparisons for each set
  combinations <- data.frame(set = factor(rep(c("Set 1", "Set 11"), each = 324)),
                      expand.grid(land1 = factor(c("Land 1", "Land 2", "Land 3", 
                                                   "Land 4", "Land 5", "Land 6")),
                                  land2 = factor(c("Land 1", "Land 2", "Land 3", 
                                                   "Land 4", "Land 5", "Land 6")),
                                  bullet1 = factor(c("Known 1", "Known 2", "Questioned")),
                                  bullet2 = factor(c("Known 1", "Known 2", "Questioned"))))
  
  # Join the two cleaned Hamby 224 sets into one testing set
  hamby224_test <- suppressWarnings(bind_rows(hamby224_set1_cleaned,
                                              hamby224_set11_cleaned)) %>%
    mutate(set = factor(set),
           bullet1 = factor(bullet1),
           bullet2 = factor(bullet2),
           land1 = factor(land1),
           land2 = factor(land2)) %>%
    right_join(combinations, by = c("set", "land1", "land2", "bullet1", "bullet2")) %>%
    filter(!(bullet1 == "Questioned" & bullet2 == "Known 1"),
           !(bullet1 == "Questioned" & bullet2 == "Known 2"),
           !(bullet1 == "Known 2" & bullet2 == "Known 1")) %>%
    arrange(rfscore) %>%
    mutate(case = factor(1:length(study))) %>%
    select(case, study:samesource)
    
  # Save the test data as a .csv file
  write.csv(hamby224_test, "../data/hamby224_test.csv", row.names = FALSE)

} else {
 
  # Read in the test data
  hamby224_test <- read.csv("../data/hamby224_test.csv")
  
}
```

#  Methods

## Random Forest Model

## Overview of LIME

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin developed LIME in response to the interpretation issue of many machine learning techniques. Many models used in machine learning are referred to as black box prediction models since they are complex models that may provide accurate predictions but are not able to be interpreted. That is, it is not possible to understand the relationship between the features and the response variable. Without being able to interpret the models, it becomes difficult to diagnose why a model is producing the predictions that it is. The developers claim that LIME allows users to check which variables are driving the predictions made by the model. This further allows users to determine whether their model is trustworthy.

LIME was designed to be a generalized algorithm that can be applied to any predictive model to return an "explanation" for an individual prediction of interest. This explanation is meant to convey which features from the model are driving the prediction created by the complex model. In the next section, I will provide a detailed description of how the LIME algorithm obtains the explanation when applied to a random forest model with numeric features. For now, general overview of the LIME algorithm will be explained.

Suppose that a complex model has been fit on a training dataset with $k$ features and a response variable that may either be numeric or categorical. This trained model will be applied to a testing dataset containing the $k$ features to obtain predictions. It is of interest to understand which variables played an important role in the predictions. LIME will be applied to one of the cases in the testing dataset to obtain an explanation. To create the explanation, LIME starts by approximating the distribution of each of the $k$ features in the training dataset. It then simulates a large data frame from these distributions. The simulated data are used to perform feature selection to determine the $m$ most important features. Then a weighted ridge regression model is fit to standardized versions of the selected $m$ features from the simulated data. The model assigns higher weights to observations closer to the observed case of interest. A linear model is used since it has a well known form and coefficients that can be interpreted. The coefficients from the model are extracted and used to "explain" the importance of the variables. Features with a larger absolute value of their coefficient are implied to play a larger role in the creation of the prediction.

## Applying LIME

The developers of LIME wrote a Python package to implement LIME, and Thomas Lin Pedersen adapted their work to create an R package called `lime`. This paper will use version `r packageVersion("lime")` of the R package `lime` in the analysis of the bullet matching data. The steps below explain the procedure that the R package is using to apply the LIME algorithm to the bullet matching predictions on the Hamby 224 clone dataset made by the random forest model from Hare. For simplicity, the steps are described as what happens to one case in the test data. Thus, the steps (2) through (7) are repeated for each observation in the testing dataset.

Let
  $$Y_{jk} = \begin{cases} 
  1 & \mbox{ if bullets } j \mbox{ and } k \mbox{ were fired from the same gun barrel }\\
  0 & \mbox{otherwise}
  \end{cases}$$
be the response variable in the training dataset, and $X_1,...,X_9$ correspond to the nine features in the training dataset. Let $X'_1,...,X'_9$ be the 

\begin{enumerate}
\item Distributions for each of the features in the training data are obtained.

\begin{quote}
The method that `lime` uses to obtain the distribution differs based on the feature type. All of the features in the Hamby datasets are numeric. For numeric features, the default option in `lime` (`quantile\_bins = TRUE`) computes the quantiles of each feature based on the number of bins selected. The default number of bins is 4 (`n\_bins = 4`).
\end{quote}

\item $n$ samples from each of the feature distributions are drawn.

\begin{quote}
To do this, `lime` has several options (mostly quoted from `lime` package for now):
\begin{itemize}
\item bin\_continuous = TRUE should continuous variables be binned?
\item quantile\_bins = TRUE should the ins for n\_bins be based on quantiles or spread evenly
\item n\_bins = 4 number of bins if bin\_continuous is TRUE
\item use\_density = TRUE if bin\_continuous is FALSE, should continuous data be sampled using kernel density estimation (if not, then will assume normal for continuous variable)
\end{itemize}
\end{quote}

\item Predictions for the testing data using the random forest model are computed.

\begin{quote}
The random forest model `rtrees` is used to make a prediction for the observation from the test dataset and each of the $n=5000$ samples as to whether or not the comparison of the two bullets in the test case are a match. Since the random forest is a classification model, `lime` is set to return the prediction probabilities.
\end{quote}

\item Similarity score between the observation in the testing data and each of the $n=5000$ sampled values are obtained.

\begin{quote}
The way that the similarity score is computed depends on the type of feature. Since all of the features in the Hamby 224 test dataset are continuous, the simulated values are first converted into 0-1 features where a 1 indicates that the feature from the simulated value falls in the same bin as the observed data point and a 0 indicates that the feature is not in the same bin as the observed data point. Then, by default, the Gower distance is used to compute the similarity score. (using the `gower` package in R)
\end{quote}

\item Feature selection is performed by fitting some type of regression model weighted by the similarity scores is to the simulated data and the observed value. The 0-1 versions of the features are used.

\begin{quote}
The user can specify the number of features, $m$, they would like to select to explain the prediction. `lime` supports the following options for feature selection
\begin{enumerate}
\item forward selection with ridge regression
\item highest weight with ridge regression
\item LASSO model
\item tree model
\item default: forward selection if $m\le6$ with a ridge regression model, highest weight with ridge regression otherwise
\end{enumerate}
\end{quote}

\item A ridge regression model is fit as the simple model by regressing the prediction probabilities on the $m$ selected predictor variables and weighted by the similarity scores. If the response is categorical, the user can select how many categories and which categories they want to explain.

  $$P(Match = TRUE) = \beta_0 + 
  \beta_1 \cdot I\left[X_1 \in \mbox{obs bin}\right] + 
  \beta_2 \cdot I\left[X_2 \in \mbox{obs bin}\right] + 
  \beta_3 \cdot I\left[X_3 \in \mbox{obs bin}\right]$$
  
For the prediction of interest, 

  $$P(Match = TRUE) = \beta_0 + \beta_1 + \beta_2 + \beta_3.$$

\item The feature weights are extracted and used as the explanations.

\end{enumerate}

Note: I realized that if `bin_continuous = FALSE`, then bins are not used at all. Instead, a kernel density estimator is used to sample from the distribution (or a normal distribution if specified), and then the ridge regression models are fit without "numerified" values.


```{r}
# Apply the run_lime function if the lime results file does not already exist
if(!file.exists("../data/hamby224_lime.rds")) {
  
  # Specify the options to use with lime 
  hamby224_lime_cases <- data.frame(bin_continuous = c(rep(TRUE, 10), rep(FALSE, 2)),
                                    quantile_bins = c(rep(TRUE, 5), rep(FALSE, 5), rep(TRUE, 2)),
                                    nbins = c(rep(2:6, 2), rep(4, 2)),
                                    use_density = c(rep(TRUE, 10), TRUE, FALSE)) %>%
  mutate(case = 1:length(nbins)) %>%
  select(case, bin_continuous, nbins, quantile_bins, use_density)
  
  # Run the lime functions for the specified number of bins
  hamby224_all_results <- pmap(.l = as.list(hamby224_lime_cases[,-1]),
             .f = run_lime, # run_lime is one of my helper functions
             train = hamby173and252_train %>% select(rf_features),
             test = hamby224_test %>% arrange(case) %>% select(rf_features) %>% na.omit(),
             rfmodel = as_classifier(rtrees),
             label = "TRUE",
             nfeatures = 3)
  
  # Separate the lime and explain function results
  hamby224_lime <- map(hamby224_all_results, function(list) list$lime)
  hamby224_explain <- map_df(hamby224_all_results, function(list) list$explain)
  
  # Name the items in the lime list
  names(hamby224_lime) <- map_chr(1:dim(hamby224_lime_cases)[1], function(case) 
      sprintf("case: bin_continuous = %s, quantile_bins = %s, nbins = %0.f, use_density = %s",
              hamby224_lime_cases$bin_continuous[case],
              hamby224_lime_cases$quantile_bins[case],
              hamby224_lime_cases$nbins[case],
              hamby224_lime_cases$use_density[case]))

  # Specify the options to use with lime with no bins
  # hamby224_lime_no_bins_cases <- data.frame(bin_continuous = rep(FALSE, 2),
  #                                           quantile_bins = rep(TRUE, 2),
  #                                           nbins = rep(4, 2),
  #                                           use_density = c(TRUE, FALSE)) %>%
  #   mutate(case = 1:length(nbins)) %>%
  #   select(case, bin_continuous, nbins, quantile_bins, use_density)
  # 
  # # Run the lime functions for the specified number of bins
  # hamby224_no_bins_results <- pmap(.l = as.list(hamby224_lime_no_bins_cases[,-1]),
  #          .f = run_lime, # run_lime is one of my helper functions
  #          train = hamby173and252_train %>% select(rf_features),
  #          test = hamby224_test %>% arrange(case) %>% select(rf_features) %>% na.omit(),
  #          rfmodel = as_classifier(rtrees),
  #          label = "TRUE",
  #          nfeatures = 3)
  
  # # Separate the lime and explain function results
  # hamby224_lime_no_bins <- map(hamby224_no_bins_results, function(list) list$lime)
  # hamby224_explain_no_bins <- map_df(hamby224_no_bins_results, function(list) list$explain)
  # 
  # # Bind the lime dataframes with bin_continuous TRUE and FALSE
  # hamby224_lime <- c(hamby224_lime, hamby224_lime_no_bins)
  # hamby224_explain <- bind_rows(hamby224_explain, hamby224_explain_no_bins)

  # Save the lime and explain results objects and the case info
  saveRDS(hamby224_lime, "../data/hamby224_lime.rds")
  saveRDS(hamby224_explain, "../data/hamby224_explain.rds")
  saveRDS(hamby224_lime_cases, "../data/hamby224_lime_cases.rds")
  
} else {
  
  # Load in the lime results object
  hamby224_lime <- readRDS("../data/hamby224_lime.rds")
  hamby224_explain <- readRDS("../data/hamby224_explain.rds")
  hamby224_lime_cases <- readRDS("../data/hamby224_lime_cases.rds")
  
}
```

```{r}
# Create a dataframe with the bins
if (!file.exists("../data/hamby224_bins.csv")) {
  
  # Create a list a dataframes with the bin boundaries and bins 
  # for the different evaluations of the lime functions
  hamby224_bin_list <- map(hamby224_lime, create_bin_data)
  
  # Save the bin boundaries and the bins as separate dataframes
  hamby224_bin_boundaries <- map(hamby224_bin_list, function(m) m$boundaries)
  hamby224_bins <- map(hamby224_bin_list, function(m) m$bins)
  
  # Save the bin boundaries and bins
  saveRDS(hamby224_bin_boundaries, "../data/hamby224_bin_boundaries.rds")
  saveRDS(hamby224_bins, "../data/hamby224_bins.rds")
  
} else {
  
  # Load in the bin boundaries and bins
  hamby224_bin_boundaries <- readRDS("../data/hamby224_bin_boundaries.rds")
  hamby224_bins <- readRDS("../data/hamby224_bins.rds")
  
}
```

```{r}
# Create the test_explain combined data if the file does not already exist
if(!file.exists("../data/hamby224_test_explain.rds")) {
  
  # Join the data and the explanations and edit and add additional variables
  # Create the feature bin labels using my function "bin_labeller"
  hamby224_test_explain <- hamby224_test %>%
    mutate(case = as.character(case)) %>%
    full_join(hamby224_explain, by = "case") %>%
    mutate(case = factor(case),
           feature_desc = factor(feature_desc),
           feature_bin = pmap_chr(list(feature = feature, 
                                 feature_value = feature_value,
                                 b_c = bin_continuous,
                                 q_b = quantile_bins,
                                 n_b = nbins,
                                 u_d = use_density),
                            .f = bin_labeller, # bin_labeller is one of my helper functions
                            bin_data = hamby224_bin_boundaries,
                            case_info = hamby224_lime_cases)) %>%
    mutate(feature = factor(feature),
           nbins = factor(nbins),
           feature_number = readr::parse_number(feature_desc),
           strictly_less = FALSE) %>%
    arrange(nbins)
  
  # Finish creating the strictly less than variable
  hamby224_test_explain$strictly_less[grep("< ", hamby224_test_explain$feature_desc)] <- TRUE
  
  # Reorder the variables of feature_desc and feature_bin for plotting purposes
  hamby224_test_explain <- hamby224_test_explain %>%
    mutate(feature_desc = reorder(feature_desc, strictly_less),
           feature_desc = reorder(feature_desc, feature_number),
           feature_desc = reorder(feature_desc, as.numeric(feature))) %>%
    select(bin_continuous:use_density, case:feature_desc, feature_bin:strictly_less, 
           data, prediction)

  # Save the combined test and explain data
  saveRDS(hamby224_test_explain, "../data/hamby224_test_explain.rds")
   
} else {
  
  # Load in the data
  hamby224_test_explain <- readRDS("../data/hamby224_test_explain.rds")
  
}
```

```{r}
# Create the lime comparison data if the file does not already exist
if(!file.exists("../data/hamby224_lime_comparisons.rds")) {
  
  # Create a data frame with the interesting information relating to the differe
  # evaluations of lime and compute the difference and mean between the rf and rr
  # model predictions
  hamby224_lime_comparisons <- hamby224_test_explain %>%
    select(-data, -prediction) %>%
    group_by(case, bin_continuous, quantile_bins, nbins, use_density) %>%
    slice(1) %>%
    ungroup() %>%
    select(bin_continuous, quantile_bins, nbins, use_density, 
           set, case, rf_features, rfscore, model_prediction, model_r2) %>%
    mutate(diff = rfscore - model_prediction,
           mean = (rfscore + model_prediction) / 2)
  
  # Save the lime comparison data frame
  saveRDS(hamby224_lime_comparisons, "../data/hamby224_lime_comparisons.rds")
  
} else {
  
  # Load in the lime comparison data frame
  hamby224_lime_comparisons <- readRDS("../data/hamby224_lime_comparisons.rds")
  
}
```

```{r}
hamby224_lime_results <- hamby224_lime_comparisons %>%
  filter(!is.na(rfscore)) %>%
  group_by(bin_continuous, quantile_bins, nbins, use_density, set) %>%
  summarise(mse = (sum(diff)^2) / length(diff),
            ave_r2 = mean(model_r2)) %>%
  arrange(set)
```

```{r}
hamby224_lime_results %>%
  group_by(set) %>%
  arrange(mse) %>%
  slice(1:2)
```

```{r}
hamby224_lime_results %>%
  mutate(bc_qb = as.character(factor(bin_continuous):factor(quantile_bins)),
         nbins_update = factor(ifelse(bin_continuous == FALSE, "0", nbins))) %>%
  ggplot(aes(x = nbins_update, y = mse, color = bc_qb)) +
  geom_point() + 
  facet_grid( ~ set) + 
  theme_bw()
```

```{r}
hamby224_lime_results %>%
  group_by(set) %>%
  arrange(ave_r2) %>%
  slice(1:2)
```

```{r}
hamby224_lime_results %>%
  ggplot(aes(x = nbins, y = ave_r2, color = quantile_bins, group = quantile_bins)) +
  geom_point() + 
  geom_line() + 
  facet_grid( ~ set) + 
  theme_bw()
```

```{r}
hamby224_lime_comparisons %>%
  filter(quantile_bins == TRUE, nbins == "8", set == "Set 1") %>%
  select(rf_features, diff, -case) %>%
  gather(key = feature, value = feature_value, rf_features) %>%
  select(feature, feature_value, diff) %>%
  ggplot(aes(x = feature_value, y = diff)) + 
  geom_point() + 
  facet_wrap(~ feature, scales = "free") + 
  geom_hline(yintercept = 0, color = "blue")
```

```{r}
hamby224_lime_comparisons %>%
  filter(quantile_bins == TRUE, nbins == "3", set == "Set 11") %>%
  select(rf_features, diff, -case) %>%
  gather(key = feature, value = feature_value, rf_features) %>%
  select(feature, feature_value, diff) %>%
  ggplot(aes(x = feature_value, y = diff)) + 
  geom_point() + 
  facet_wrap(~ feature, scales = "free") + 
  geom_hline(yintercept = 0, color = "blue")
```

```{r fig.width = 8, fig.height = 4}
hamby224_lime_comparisons %>%
  filter(!is.na(rfscore), set == "Set 1") %>%
  ggplot(aes(x = rfscore, y = model_prediction)) + 
  geom_point(aes(color = model_r2)) + 
  facet_wrap( ~ nbins) + 
  theme_bw() + 
  stat_smooth(method = "lm")
```

```{r fig.width = 8, fig.height = 4}
hamby224_lime_comparisons %>%
  filter(!is.na(rfscore), set == "Set 11") %>%
  ggplot(aes(x = rfscore, y = model_prediction)) + 
  geom_point(aes(color = model_r2)) + 
  facet_wrap( ~ nbins) + 
  theme_bw() + 
  stat_smooth(method = "lm")
```

```{r fig.width = 8, fig.height = 4}
hamby224_lime_comparisons %>%
  filter(!is.na(rfscore), set == "Set 1") %>%
  ggplot(aes(x = rfscore, y = abs(diff))) + 
  geom_point(aes(color = model_r2)) + 
  facet_wrap( ~ nbins) +
  theme_bw()
```

```{r fig.width = 8, fig.height = 4}
hamby224_lime_comparisons %>%
  filter(!is.na(rfscore), set == "Set 11") %>%
  ggplot(aes(x = rfscore, y = abs(diff))) + 
  geom_point(aes(color = model_r2)) + 
  facet_wrap( ~ nbins) +
  theme_bw()
```

```{r fig.width = 8, fig.height = 4}
hamby224_lime_comparisons %>%
  filter(!is.na(rfscore), set == "Set 1") %>%
  ggplot(aes(x = rfscore, y = diff)) + 
  geom_point(aes(color = model_r2)) + 
  facet_wrap( ~ nbins) +
  geom_hline(yintercept = 0, color = "blue") +
  theme_bw()
```

```{r fig.width = 8, fig.height = 4}
hamby224_lime_comparisons %>%
  filter(!is.na(rfscore), set == "Set 11") %>%
  ggplot(aes(x = rfscore, y = diff)) + 
  geom_point(aes(color = model_r2)) + 
  facet_wrap( ~ nbins) +
  geom_hline(yintercept = 0, color = "blue") +
  theme_bw()
```

## Visualizing the LIME Explanations

```{r}
hamby224_test_explain %>%
  ggplot(aes(x = feature_desc)) + 
  geom_bar() +
  coord_flip() + 
  facet_grid(set ~ nbins)
```

```{r}
hamby224_test_explain %>%
  filter(set == "Set 1", nbins == "8") %>%
  mutate(rfscore = round(rfscore, 3)) %>%
  select(case, bullet1, bullet2, land1, land2, rfscore) %>%
  distinct() %>%
  bind_rows(hamby224_test_explain %>%
                  filter(is.na(rfscore), set == "Set 1") %>%
                  select(case, bullet1, bullet2, land1, land2, rfscore)) %>%
  ggplot(aes(x = land1, y = land2, label = bullet1, label2 = bullet2,
             text = paste('Bullets Compared: ', bullet1, "-", land1, 
                          "vs", bullet2, "-", land2,
                          '\nRandom Forest Score: ', 
                          ifelse(is.na(rfscore), "Missing due to tank rash", rfscore)))) +
  geom_tile(aes(fill = rfscore)) +
  facet_grid(bullet2 ~ bullet1, scales = "free") +
  theme_minimal() +
  scale_fill_gradient2(low = "darkgrey", high = "darkorange", midpoint = 0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "", y = "", fill = "RF Score")
```

# Results

# Discussion

Ideas for improvement of LIME: 
  - change the binning method:
    - lime is not good with linear relationships with classifiers due to the 
    inside-outside binning - it would be better to use a cumulative approach
  - consider interactions in the ridge regression model
  - learn lambda in the ridge regression
  - could add a penalty for the number of parameters in the model (or the number of bins)
  - could start with a fine grid of bins and then go backwards and fit the models on this sampled data
  - could try a type of ANOVA test if we can have nested models or if we can assume nesting
  - could compute something like an R^2_adj

# References

