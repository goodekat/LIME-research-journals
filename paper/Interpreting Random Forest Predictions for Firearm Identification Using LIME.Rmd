---
title: "Interpreting Random Forest Predictions for Firearm Identification Using LIME"
author: "Katherine Goode and Heike Hofmann"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: true
bibliography: references.bib
---

```{r setup, include = FALSE}
# Set knitr options
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, 
                      fig.pos = "center")

# Load libraries
library(bulletr)
library(cowplot)
library(furrr)
library(future)
library(lime)
library(purrr)
library(randomForest)
library(tidyverse)
library(tree)

# Source functions
source("../code/helper_functions.R")
```

# Introduction

(need to finish adding sources)

The discipline of firearm identification examines bullets to determine the likelihood that a bullet found in a criminal case was fired from a particular gun. To do this, the bullet from the crime will be compared with a bullet that was known to be fired from the gun under evaluation. Traditionally, this is a procedure that has been performed by hand. Specially trained examiners visually compare the microscopic bullet striations that were created when the bullets passed through the gun barrel. Often a comparison microscope is used that allows the examiners to view both bullets at the same time (National Research Council 2009). The examiners use this ability to determine whether the two bullets were fired from the same gun barrel.

Recently, the scientific community has been encouraging the inclusion of more data driven techniques to be used in forensic investigations. These methods would allow for the reporting of a measure of uncertainty in addition to the conclusion drawn from the analysis. This led Hare, Hofmann, and Carriquiry (2017) to propose a new computer automated method of bullet matching that could supplement the visual inspection by the firearm examiners. Their method involves obtaining bullet signatures of the striations from the scans of the two bullets, computing variables that measure the similarity of the two signatures, and using a trained random forest model to determine the probability of a match based on the similarity variables. They trained their model on a set of known bullet matches and non-matches from the Hamby study. They demonstrated how the random forest model can be used to make prediction on a new set of bullet signature comparisons. The results from the paper suggest that the random forest model leads to highly accurate bullet matching predictions. The authors found that when their model was applied to a testing dataset (?), the resulting error rate was 0%.

While random forest models often result in good predictions as seen in Hare, Hofmann, and Carriquiry, it is well known that a disadvantage of random forest models and other machine learning techniques is that it is difficult to interpret the models (reference?). For example, it is not possible to tell which variables played an important role in the creation of individual predictions. This issue led to the development of LIME (reference), which is an algorithm that examines the behavior of the complicated model on a local scale around a new prediction using a linear regression model. This allows for the ability to understand which were the driving variables that led to a prediction of interest.

(should probably list other current methods for interpreting random forests)

Since firearm identification identification is commonly used as evidence for convictions in court cases, it is important to be able to understand and assess the model that is being used to quantify the probability that a bullet was fired from a gun. LIME provides the ability to understand which were the key variables used by the random forest model to make a prediction, which would allow firearm examiners to check whether or not the predictions created by the random forest are based on reasonable variables. 

This paper provides an example of the application of LIME to a bullet matching problem. (provide more details about what is contained in the papers - i.e. section 2 describes the Hamby data; section 3 describes the random forest model and LIME; ...)

# Data

## Training Data: The Hamby 173 and 252 Datasets

```{r}
# Load in the training data (Hamby Data 173 and 252)
hamby173and252_train <- read.csv("../data/hamby173and252_train.csv")
```

```{r}
# Obtain features used when fitting the rtrees random forest
rf_features <- rownames(rtrees$importance)

# Add the random forest score to the training set
hamby173and252_train$rfscore = predict(rtrees, 
                                       hamby173and252_train %>% select(rf_features), 
                                       type = "prob")[,2]
```

## Testing Data: The Hamby 224 Clone

The Hamby 224 Clone is organized as a test set of a cloned (sub-)set of the Hamby 224 bullets. As with all Hamby sets [@hamby], Hamby set 224, is a collection of 35 bullets, organized as 20 known bullets and 15 questioned bullets. The known bullets are fired in pairs of two through one of ten consecutively manufactures P-85 barrels.
Clone set 224 is arranged as a test set of fifteen tests, one for each questioned bullet. Each test set is arranged as a combination of three bullets: two known bullets and a questioned bullet. The test asks for a decision on whether the questioned bullet comes from the same source as the two known bullets  or from a different source. This situation is similar to what a firearms and toolmarks examiner might encounter in case work. 

```{r}
# Load in the Hamby 224 datasets
hamby224_set1 <- readRDS("../data/h224-set1-features.rds")
hamby224_set11 <- readRDS("../data/h224-set11-features.rds")
```

```{r}
# If the test data does not exist, clean the data, create the test data, and save it
# Otherwise, load in the test data
if(!file.exists("../data/hamby224_test.csv")) {
  
  # Clean the Hamby 224 set 1 data
  hamby224_set1_cleaned <- hamby224_set1 %>%
    select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
    rename(bullet1 = bulletA,
           bullet2 = bulletB, 
           land1 = landA,
           land2 = landB) %>%
    mutate(study = factor("Hamby 224"), 
           set = factor("Set 1"),
           bullet1 = recode(factor(bullet1), 
                            "1" = "Known 1", "2" = "Known 2", "Q" = "Questioned"),
           bullet2 = recode(factor(bullet2), 
                            "1" = "Known 1", "2" = "Known 2", "Q" = "Questioned"),
           land1 = recode(factor(land1), 
                          "1" = "Land 1", "2" = "Land 2", "3" = "Land 3", 
                          "4" = "Land 4", "5" = "Land 5", "6" = "Land 6"),
           land2 = recode(factor(land2), 
                          "1" = "Land 1", "2" = "Land 2", "3" = "Land 3", 
                          "4" = "Land 4", "5" = "Land 5", "6" = "Land 6")) %>%
    select(study, set, bullet1:land2, rf_features, rfscore, samesource)

  # Clean the Hamby 224 set 11 data
  hamby224_set11_cleaned <- hamby224_set11 %>%
    select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
    rename(bullet1 = bulletA,
           bullet2 = bulletB, 
           land1 = landA,
           land2 = landB) %>%
    mutate(study = factor("Hamby 224"), 
           set = factor("Set 11"),
           bullet1 = recode(factor(bullet1), 
                            "Bullet 1" = "Known 1", "Bullet 2" = "Known 2", 
                            "Bullet I" = "Questioned"),
           bullet2 = recode(factor(bullet2), 
                            "Bullet 1" = "Known 1", "Bullet 2" = "Known 2", 
                            "Bullet I" = "Questioned")) %>%
    select(study, set, bullet1:land2, rf_features, rfscore, samesource)

  # Create a dataset with all combinations of lands and bullets comparisons for each set
  combinations <- data.frame(set = factor(rep(c("Set 1", "Set 11"), each = 324)),
                      expand.grid(land1 = factor(c("Land 1", "Land 2", "Land 3", 
                                                   "Land 4", "Land 5", "Land 6")),
                                  land2 = factor(c("Land 1", "Land 2", "Land 3", 
                                                   "Land 4", "Land 5", "Land 6")),
                                  bullet1 = factor(c("Known 1", "Known 2", "Questioned")),
                                  bullet2 = factor(c("Known 1", "Known 2", "Questioned"))))
  
  # Join the two cleaned Hamby 224 sets into one testing set
  hamby224_test <- suppressWarnings(bind_rows(hamby224_set1_cleaned,
                                              hamby224_set11_cleaned)) %>%
    mutate(set = factor(set),
           bullet1 = factor(bullet1),
           bullet2 = factor(bullet2),
           land1 = factor(land1),
           land2 = factor(land2)) %>%
    right_join(combinations, by = c("set", "land1", "land2", "bullet1", "bullet2")) %>%
    filter(!(bullet1 == "Questioned" & bullet2 == "Known 1"),
           !(bullet1 == "Questioned" & bullet2 == "Known 2"),
           !(bullet1 == "Known 2" & bullet2 == "Known 1")) %>%
    arrange(rfscore) %>%
    mutate(case = factor(1:length(study))) %>%
    select(case, study:samesource)
    
  # Save the test data as a .csv file
  write.csv(hamby224_test, "../data/hamby224_test.csv", row.names = FALSE)

} else {
 
  # Read in the test data
  hamby224_test <- read.csv("../data/hamby224_test.csv")
  
}
```

#  Methods

## Random Forest Model

## Overview of LIME

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin developed LIME in response to the interpretation issue of many machine learning techniques. Many models used in machine learning are referred to as black box prediction models since they are complex models that may provide accurate predictions but are not able to be interpreted. That is, it is not possible to understand the relationship between the features and the response variable. Without being able to interpret the models, it becomes difficult to diagnose why a model is producing the predictions that it is. The developers claim that LIME allows users to check which variables are driving the predictions made by the model. This further allows users to determine whether their model is trustworthy.

LIME was designed to be a generalized algorithm that can be applied to any predictive model to return an "explanation" for an individual prediction of interest. This explanation is meant to convey which features from the model are driving the prediction created by the complex model. In the next section, I will provide a detailed description of how the LIME algorithm obtains the explanation when applied to a random forest model with numeric features. For now, general overview of the LIME algorithm will be explained.

Suppose that a complex model has been fit on a training dataset with $k$ features and a response variable that may either be numeric or categorical. This trained model will be applied to a testing dataset containing the $k$ features to obtain predictions. It is of interest to understand which variables played an important role in the predictions. LIME will be applied to one of the cases in the testing dataset to obtain an explanation. To create the explanation, LIME starts by approximating the distribution of each of the $k$ features in the training dataset. It then simulates a large data frame from these distributions. The simulated data are used to perform feature selection to determine the $m$ most important features. Then a weighted ridge regression model is fit to standardized versions of the selected $m$ features from the simulated data. The model assigns higher weights to observations closer to the observed case of interest. A linear model is used since it has a well known form and coefficients that can be interpreted. The coefficients from the model are extracted and used to "explain" the importance of the variables. Features with a larger absolute value of their coefficient are implied to play a larger role in the creation of the prediction.

The developers of LIME wrote a Python package to implement LIME, and Thomas Lin Pedersen adapted their work to create an R package called `lime`. This paper will use version `r packageVersion("lime")` of the R package `lime` in the analysis of the bullet matching data.


## Applying LIME

(this is the section where I will describe how lime was applied and which input values were used)

```{r}
# Apply the run_lime function if the lime results file does not already exist
if(!file.exists("../data/hamby224_lime_inputs.rds")) {
  
  # Specify the options to use with lime
  hamby224_lime_inputs <- list(bin_continuous = c(rep(TRUE, 20), rep(FALSE, 2)),
                               quantile_bins = c(rep(TRUE, 5), rep(FALSE, 5), 
                                                 rep(TRUE, 10), rep(TRUE, 2)),
                               nbins = c(rep(2:6, 4), rep(4, 2)),
                               use_density = c(rep(TRUE, 20), TRUE, FALSE),
                               bin_method = c(rep("quantile_bins", 5),
                                              rep("equally_spaced", 5),
                                              rep("tree", 10),
                                              rep("quantile_bins", 2)),
                               response = c(rep(NA, 10), rep("samesource", 5), 
                                            rep("rfscore", 5), rep(NA, 2)))
  
  # Tell R to run the upcoming code in parallel
  plan(multiprocess)
  
  # Apply lime to the full training data with the specified input options
  hamby224_lime_explain <- future_pmap(.l = hamby224_lime_inputs,
             .f = run_lime, # run_lime is one of my helper functions
             features = rf_features,
             train = hamby173and252_train,
             test = hamby224_test %>% arrange(case) %>% select(rf_features) %>% na.omit(),
             rfmodel = as_classifier(rtrees),
             label = "TRUE",
             nfeatures = 3,
             seed = TRUE)
  
  # Separate the lime and explain function results from the full data
  hamby224_lime <- map(hamby224_lime_explain, function(list) list$lime)
  hamby224_explain <- map_df(hamby224_lime_explain, function(list) list$explain)
  
  # Name the items in the lime list
  names(hamby224_lime) <- map_chr(1:length(hamby224_lime_inputs), function(case) 
      sprintf("case: bin_continuous = %s, quantile_bins = %s, nbins = %0.f, use_density = %s, bin_method = %s, response = %s",
              hamby224_lime_inputs$bin_continuous[case],
              hamby224_lime_inputs$quantile_bins[case],
              hamby224_lime_inputs$nbins[case],
              hamby224_lime_inputs$use_density[case],
              hamby224_lime_inputs$bin_method[case],
              hamby224_lime_inputs$response[case]))

  hamby224_lime_inputs <- hamby224_lime_inputs %>%
    unlist() %>%
    matrix(ncol = length(hamby224_lime_inputs), 
           dimnames = list(NULL, names(hamby224_lime_inputs))) %>%
    as.data.frame() %>%
    mutate(case = 1:length(hamby224_lime_inputs$quantile_bins)) %>%
    select(case, bin_continuous:response)
  
  # Save the lime objects
  saveRDS(hamby224_lime_inputs, "../data/hamby224_lime_inputs.rds")
  saveRDS(hamby224_lime, "../data/hamby224_lime.rds")
  saveRDS(hamby224_explain, "../data/hamby224_explain.rds")
  
} else {
  
  # Load in the lime objects
  hamby224_lime_inputs <- readRDS("../data/hamby224_lime_inputs.rds")
  hamby224_lime <- readRDS("../data/hamby224_lime.rds")
  hamby224_explain <- readRDS("../data/hamby224_explain.rds")
  
}
```

```{r}
# Create a dataframe with the bins
if (!file.exists("../data/hamby224_bins.csv")) {
  
  # Create a list a dataframes with the bin boundaries and bins 
  # for the different evaluations of the lime functions
  hamby224_bin_list <- map(hamby224_lime, create_bin_data)
  
  # Save the bin boundaries and the bins as separate dataframes
  hamby224_bin_boundaries <- map(hamby224_bin_list, function(m) m$boundaries)
  hamby224_bins <- map(hamby224_bin_list, function(m) m$bins)
  
  # Save the bin boundaries and bins
  saveRDS(hamby224_bin_boundaries, "../data/hamby224_bin_boundaries.rds")
  saveRDS(hamby224_bins, "../data/hamby224_bins.rds")
  
} else {
  
  # Load in the bin boundaries and bins
  hamby224_bin_boundaries <- readRDS("../data/hamby224_bin_boundaries.rds")
  hamby224_bins <- readRDS("../data/hamby224_bins.rds")
  
}
```

```{r}
# Create the test_explain combined data if the file does not already exist
if(!file.exists("../data/hamby224_test_explain.rds")) {
  
  # Join the data and the explanations and edit and add additional variables
  # Create the feature bin labels using my function "bin_labeller"
  hamby224_test_explain <- hamby224_test %>%
    mutate(case = as.character(case)) %>%
    full_join(hamby224_explain, by = "case") %>%
    mutate(case = factor(case),
           feature_desc = factor(feature_desc),
           feature_bin = pmap_chr(list(feature = feature, 
                                  feature_value = feature_value,
                                  b_c = bin_continuous,
                                  q_b = quantile_bins,
                                  n_b = nbins,
                                  u_d = use_density,
                                  b_m = bin_method,
                                  r_v = response),
                            .f = bin_labeller, # bin_labeller is one of my helper functions
                            bin_data = hamby224_bin_boundaries,
                            case_info = hamby224_lime_inputs)) %>%
    mutate(feature = factor(feature),
           nbins = factor(nbins),
           feature_number = readr::parse_number(feature_desc),
           strictly_less = FALSE) %>%
    arrange(nbins)
  
  # Finish creating the strictly less than variable
  hamby224_test_explain$strictly_less[grep("< ", hamby224_test_explain$feature_desc)] <- TRUE
  
  # Reorder the variables of feature_desc and feature_bin for plotting purposes and
  # create new variables of situation and bin_situation
  hamby224_test_explain <- hamby224_test_explain %>%
    mutate(feature_desc = reorder(feature_desc, strictly_less),
           feature_desc = reorder(feature_desc, feature_number),
           feature_desc = reorder(feature_desc, as.numeric(feature))) %>%
    mutate(nbins = as.numeric(as.character(nbins)),
           situation = ifelse(bin_continuous == TRUE & bin_method == "quantile_bins", 
                              sprintf("%.0f quantile bins", nbins),
                              ifelse(bin_continuous == TRUE & bin_method == "equally_spaced",
                                     sprintf("%.0f equally spaced bins", nbins),
                                     ifelse(bin_continuous == TRUE & bin_method == "tree" &
                                              response == "samesource",
                                            sprintf("%.0f samesource tree based bins", nbins),
                                            ifelse(bin_continuous == TRUE & bin_method == "tree" &
                                              response == "rfscore",
                                              sprintf("%.0f rfscore tree based bins", nbins),
                                              ifelse(bin_continuous == FALSE & 
                                                       use_density == TRUE, 
                                                     "kernel density", "normal approximation"))))) %>%
             fct_relevel("2 quantile bins", "3 quantile bins", "4 quantile bins",
                         "5 quantile bins", "6 quantile bins", "2 equally spaced bins",
                         "3 equally spaced bins", "4 equally spaced bins",
                         "5 equally spaced bins", "6 equally spaced bins",
                         "2 samesource tree based bins", "3 samesource tree based bins",
                         "4 samesource tree based bins", "5 samesource tree based bins",
                         "6 samesource tree based bins")) %>%
    mutate(bin_situation = ifelse(bin_method == "quantile_bins" & bin_continuous == TRUE,
                                "quantile bins",
                                ifelse(bin_method == "equally_spaced" & bin_continuous == TRUE,
                                       "equally spaced bins", 
                                       ifelse(bin_method == "tree" & bin_continuous == TRUE & 
                                                response == "samesource",
                                              "samesource tree based bins", 
                                              ifelse(bin_method == "tree" & bin_continuous == TRUE & 
                                                response == "rfscore",
                                                "rfscore tree based bins", "other"))))) %>%
    mutate(bin_situation = factor(bin_situation)) %>%
    select(situation, bin_situation, bin_continuous:response, case:feature_desc,
           feature_bin:strictly_less, data, prediction)

  # Save the combined test and explain data
  saveRDS(hamby224_test_explain, "../data/hamby224_test_explain.rds")
   
} else {
  
  # Load in the data
  hamby224_test_explain <- readRDS("../data/hamby224_test_explain.rds")
  
}
```

```{r}
# Create the lime comparison data if the file does not already exist
if(!file.exists("../data/hamby224_lime_comparisons.rds")) {
  
  # Create a data frame with the interesting information relating to the different
  # evaluations of lime and compute the difference and mean between the rf and rr
  # model predictions
  hamby224_lime_comparisons <- hamby224_test_explain %>%
    select(-data, -prediction) %>%
    group_by(case, bin_continuous, quantile_bins, nbins, use_density, bin_method, response) %>%
    slice(1) %>%
    ungroup() %>%
    select(situation, bin_situation, bin_method, bin_continuous, quantile_bins, response, 
           nbins, use_density, set, case, rf_features, rfscore, model_prediction, model_r2) %>%
    mutate(diff = rfscore - model_prediction,
           mean = (rfscore + model_prediction) / 2)
    
  
  # Save the lime comparison data frame
  saveRDS(hamby224_lime_comparisons, "../data/hamby224_lime_comparisons.rds")
  
} else {
  
  # Load in the lime comparison data frame
  hamby224_lime_comparisons <- readRDS("../data/hamby224_lime_comparisons.rds")
  
}
```

## Development of Shiny App for Visualizing the LIME Explanations

# Results

- show examples of using the app and what we have learned from the LIME explanations

# Discussion

Ideas for improvement of LIME: 
  - change the binning method:
    - lime is not good with linear relationships with classifiers due to the 
    inside-outside binning - it would be better to use a cumulative approach
  - consider interactions in the ridge regression model
  - learn lambda in the ridge regression
  - could add a penalty for the number of parameters in the model (or the number of bins)
  - could start with a fine grid of bins and then go backwards and fit the models on this sampled data
  - could try a type of ANOVA test if we can have nested models or if we can assume nesting
  - could compute something like an R^2_adj
  - could force the intercept to be 0.5
  - try out the subsampling

# References

