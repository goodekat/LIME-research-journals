---
title: "Interpreting Random Forest Predictions for Firearm Identification Using LIME"
author: "Katherine Goode and Heike Hofmann"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: true
bibliography: references.bib
---

```{r setup, include = FALSE}
# Set knitr options
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE)

# Load libraries
library(tidyverse)
library(bulletr)
library(lime)
library(randomForest)
```

# Introduction

(need to finish adding sources)

The discipline of firearm identification examines bullets to determine the likelihood that a bullet found in a criminal case was fired from a particular gun. To do this, the bullet from the crime will be compared with a bullet that was known to be fired from the gun under evaluation. Traditionally, this is a procedure that has been performed by hand. Specially trained examiners visually compare the microscopic bullet striations that were created when the bullets passed through the gun barrel. Often a comparison microscope is used that allows the examiners to view both bullets at the same time (National Research Council 2009). The examiners use this ability to determine whether the two bullets were fired from the same gun barrel.

Recently, the scientific community has been encouraging the inclusion of more data driven techniques to be used in forensic investigations. These methods would allow for the reporting of a measure of uncertainty in addition to the conclusion drawn from the analysis. This led Hare, Hofmann, and Carriquiry (2017) to propose a new computer automated method of bullet matching that could supplement the visual inspection by the firearm examiners. Their method involves obtaining bullet signatures of the striations from the scans of the two bullets, computing variables that measure the similarity of the two signatures, and using a trained random forest model to determine the probability of a match based on the similarity variables. They trained their model on a set of known bullet matches and non-matches from the Hamby study. They demonstrated how the random forest model can be used to make prediction on a new set of bullet signature comparisons. The results from the paper suggest that the random forest model leads to highly accurate bullet matching predictions. The authors found that when their model was applied to a testing dataset (?), the resulting error rate was 0%.

While random forest models often result in good predictions as seen in Hare, Hofmann, and Carriquiry, it is well known that a disadvantage of random forest models and other machine learning techniques is that it is difficult to interpret the models (reference?). For example, it is not possible to tell which variables played an important role in the creation of individual predictions. This issue led to the development of LIME (reference), which is an algorithm that examines the behavior of the complicated model on a local scale around a new prediction using a linear regression model. This allows for the ability to understand which were the driving variables that led to a prediction of interest.

Since firearm identification identification is commonly used as evidence for convictions in court cases, it is important to be able to understand and assess the model that is being used to quantify the probability that a bullet was fired from a gun. LIME provides the ability to understand which were the key variables used by the random forest model to make a prediction, which would allow firearm examiners to check whether or not the predictions created by the random forest are based on reasonable variables. 

This paper provides an example of the application of LIME to a bullet matching problem. (provide more details about what is contained in the papers - i.e. section 2 describes the Hamby data; section 3 describes the random forest model and LIME; ...)

# Data

## Training Data: The Hamby 173 and 252 Datasets

```{r}
# Load in the training data (Hamby Data 173 and 252)
hamby173and252_train <- read.csv("../data/hamby173and252_train.csv")
```

## Testing Data: The Hamby 224 Clone

The Hamby 224 Clone is organized as a test set of a cloned (sub-)set of the Hamby 224 bullets. As with all Hamby sets [@hamby], Hamby set 224, is a collection of 35 bullets, organized as 20 known bullets and 15 questioned bullets. The known bullets are fired in pairs of two through one of ten consecutively manufactures P-85 barrels.
Clone set 224 is arranged as a test set of fifteen tests, one for each questioned bullet. Each test set is arranged as a combination of three bullets: two known bullets and a questioned bullet. The test asks for a decision on whether the questioned bullet comes from the same source as the two known bullets  or from a different source. This situation is similar to what a firearms and toolmarks examiner might encounter in case work. 

```{r}
# Load in the Hamby 224 datasets
hamby224_set1 <- readRDS("../data/h224-set1-features.rds")
hamby224_set11 <- readRDS("../data/h224-set11-features.rds")
```

```{r}
# Obtain features used when fitting the rtrees random forest
rf_features <- rownames(rtrees$importance)

# If the test data does not exist, clean the data, create the test data, and save it
# Otherwise, load in the test data
if(!file.exists("../data/hamby224_test.csv")) {
  
  # Clean the Hamby 224 set 1 data
  hamby224_set1_cleaned <- hamby224_set1 %>%
    select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
    rename(bullet1 = bulletA,
           bullet2 = bulletB, 
           land1 = landA,
           land2 = landB) %>%
    mutate(study = factor("Hamby 224"), 
           set = factor("Set 1"),
           bullet1 = recode(factor(bullet1), "1" = "Known 1", "2" = "Known 2", "Q" = "Questioned"),
           bullet2 = recode(factor(bullet2), "1" = "Known 1", "2" = "Known 2", "Q" = "Questioned"),
           land1 = recode(factor(land1), "1" = "Land 1", "2" = "Land 2", "3" = "Land 3", 
                          "4" = "Land 4", "5" = "Land 5", "6" = "Land 6"),
           land2 = recode(factor(land2), "1" = "Land 1", "2" = "Land 2", "3" = "Land 3", 
                          "4" = "Land 4", "5" = "Land 5", "6" = "Land 6")) %>%
    select(study, set, bullet1:land2, rf_features, rfscore, samesource)

  # Clean the Hamby 224 set 11 data
  hamby224_set11_cleaned <- hamby224_set11 %>%
    select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
    rename(bullet1 = bulletA,
           bullet2 = bulletB, 
           land1 = landA,
           land2 = landB) %>%
    mutate(study = factor("Hamby 224"), 
           set = factor("Set 11"),
           bullet1 = recode(factor(bullet1), "Bullet 1" = "Known 1", "Bullet 2" = "Known 2", 
                            "Bullet I" = "Questioned"),
           bullet2 = recode(factor(bullet2), "Bullet 1" = "Known 1", "Bullet 2" = "Known 2", 
                            "Bullet I" = "Questioned")) %>%
    select(study, set, bullet1:land2, rf_features, rfscore, samesource)

  # Create a dataset with all combinations of lands and bullets comparisons for each set
  combinations <- data.frame(set = factor(rep(c("Set 1", "Set 11"), each = 324)),
                      expand.grid(land1 = factor(c("Land 1", "Land 2", "Land 3", 
                                                   "Land 4", "Land 5", "Land 6")),
                                  land2 = factor(c("Land 1", "Land 2", "Land 3", 
                                                   "Land 4", "Land 5", "Land 6")),
                                  bullet1 = factor(c("Known 1", "Known 2", "Questioned")),
                                  bullet2 = factor(c("Known 1", "Known 2", "Questioned"))))
  
  # Join the two cleaned Hamby 224 sets into one testing set
  hamby224_test <- suppressWarnings(bind_rows(hamby224_set1_cleaned,
                                              hamby224_set11_cleaned)) %>%
    mutate(set = factor(set),
           bullet1 = factor(bullet1),
           bullet2 = factor(bullet2),
           land1 = factor(land1),
           land2 = factor(land2)) %>%
    right_join(combinations, by = c("set", "land1", "land2", "bullet1", "bullet2")) %>%
    filter(!(bullet1 == "Questioned" & bullet2 == "Known 1"),
           !(bullet1 == "Questioned" & bullet2 == "Known 2"),
           !(bullet1 == "Known 2" & bullet2 == "Known 1")) %>%
    arrange(rfscore) %>%
    mutate(case = factor(1:length(study))) %>%
    select(case, study:samesource)
    
    
  # Save the test data as a .csv file
  write.csv(hamby224_test, "../data/hamby224_test.csv", row.names = FALSE)

} else {
 
  # Read in the test data
  hamby224_test <- read.csv("../data/hamby224_test.csv")
  
}
```

#  Methods

## Random Forest Model



## Overview of LIME

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin developed LIME in response to the interpretabiliby issue of many machine learning techniques. Many models used in machine learning are referred to as black box prediction models since they are complex models that may provide accurate predictions but are not able to be interpreted. That is, it is not possible to understand the relationship between the features and the response variable. Without being able to interpret the models, it becomes difficult to diagnose why a model is producing the predictions that it is. The developers claim that LIME allows users to check which variables are driving the predictions made by the model. This further allows users to determine whether their model is trustworthy.

LIME was designed to be a generalized algorithm that can be applied to any predictive model to return an "explanation" for an individual prediction of interest. This explanation is meant to convey which features from the model are driving the prediction created by the complex model. In the next section, I will provide a detailed description of how the LIME algorithm obtains the explanation when applied to a random forest model with numeric features. For now, general overview of the LIME algorithm will be explained.

Suppose that a complex model has been fit on a training dataset with $k$ features and a response variable that may either be numeric or categorical. This trained model will be applied to a testing dataset containing the $k$ features to obtain predictions. It is of interest to understand which variables played an important role in the predictions. LIME will be applied to one of the cases in the testing dataset to obtain an explanation. To create the explanation, LIME starts by approximating the distribution of each of the $k$ features in the training dataset. It then simulates a large dataframe from these distributions. The simulated data are used to perform feature selection to determine the $m$ most important features. Then a weighted ridge regression model is fit to standardized versions of the selected $m$ features from the simulated data. The model assigns higher weights to observations closer to the observed case of interest. A linear model is used since it has a well known form and coefficients that can be interpreted. The coefficients from the model are extracted and used to "explain" the importance of the variables. Features with a larger absolute value of their coefficient are implied to play a larger role in the creation of the prediction.

## Applying LIME

The developers of LIME wrote a Python package to implement LIME, and Thomas Lin Pedersen adapted their work to create an R package called `lime`. This paper will use version `r packageVersion("lime")` of the R package `lime` in the analysis of the bullet matching data. The steps below explain the procedure that the R package is using to apply the LIME algorithm to the bullet matching predictions on the Hamby 224 clone dataset made by the random forest model from Hare. For simplicity, the steps are described as what happens to one case in the test data. Thus, the steps (2) through (7) are repeated for each observation in the testing dataset.

Let
  $$Y_{jk} = \begin{cases} 
  1 & \mbox{ if bullets } j \mbox{ and } k \mbox{ were fired from the same gun barrel }\\
  0 & \mbox{otherwise}
  \end{cases}$$
be the response variable in the training dataset, and $X_1,...,X_9$ correspond to the nine features in the training dataset. Let $X'_1,...,X'_9$ be the 


\begin{enumerate}
\item Distributions for each of the features in the training data are obtained.

\begin{quote}
The method that `lime` uses to obtain the distribution differs based on the feature type. All of the features in the Hamby datasets are numeric. For numeric features, the default option in `lime` (`quantile\_bins = TRUE`) computes the quantiles of each feature based on the number of bins selected. The default number of bins is 4 (`n\_bins = 4`).
\end{quote}

\item $n$ samples from each of the feature distributions are drawn.

\begin{quote}
To do this, `lime` draws a sample from a Uniform(0,1) distribution and uses the bin information to compute a sample drawn from the feature distribution.
\end{quote}

\item Predictions for the testing data using the random forest model are computed.

\begin{quote}
The random forest model `rtrees` is used to make a prediction for the observation from the test dataset and each of the $n=5000$ samples as to whether or not the comparison of the two bullets in the test case are a match. Since the random forest is a classification model, `lime` is set to return the prediction probabilities.
\end{quote}

\item Similarity score between the observation in the testing data and each of the $n=5000$ sampled values are obtained.

\begin{quote}
The way that the similarity score is computed depends on the type of feature. Since all of the features in the Hamby 224 test dataset are continuous, the simulated values are first converted into 0-1 features where a 1 indicates that the feature from the simulated value falls in the same bin as the observed data point and a 0 indicates that the feature is not in the same bin as the observed data point. Then, by default, the Gower distance is used to compute the similarity score. (using the `gower` package in R)
\end{quote}

\item Feature selection is performed by fitting some type of regression model weighted by the similarity scores is to the simulated data and the observed value. The 0-1 versions of the features are used.

\begin{quote}
The user can specify the number of features, $m$, they would like to select to explain the prediction. `lime` supports the following options for feature selection
\begin{enumerate}
\item forward selection with ridge regression
\item highest weight with ridge regression
\item LASSO model
\item tree model
\item default: forward selection if $m\le6$ with a ridge regression model, highest weight with ridge regression otherwise
\end{enumerate}
\end{quote}

\item A ridge regression model is fit as the simple model by regressing the prediction probabilities on the $m$ selected predictor variables and weighted by the similarity scores. If the response is categorical, the user can select how many categories and which categories they want to explain.

  $$P(Match = TRUE) = \beta_0 + 
  \beta_1 \cdot I\left[X_1 \in \mbox{obs bin}\right] + 
  \beta_2 \cdot I\left[X_2 \in \mbox{obs bin}\right] + 
  \beta_3 \cdot I\left[X_3 \in \mbox{obs bin}\right]$$
  
For the prediction of interest, 

  $$P(Match = TRUE) = \beta_0 + \beta_1 + \beta_2 + \beta_3.$$

\item The feature weights are extracted and used as the explanations.

\end{enumerate}

```{r}
# Apply the lime function if the file does not already exist
if(!file.exists("../data/hamby224_lime.rds")) {
  
  # Apply the lime function from the lime package (with a seed set)
  # Note that the as_classifier must be added since rtrees is from the
  # randomForest package and not fit using caret or one of the other 
  # available models specified in the lime package. Additionally, the
  # randomForest package must be loaded in order to run the functions
  # from the lime package. (I should check on exactly how this works.)
  set.seed(84902)
  hamby224_lime <- lime(x = hamby173and252_train %>% select(rf_features),
                        model = as_classifier(rtrees))
  
  # Save the lime object
  saveRDS(hamby224_lime, "../data/hamby224_lime.rds")

} else {
  
  # Load in the lime object
  hamby224_lime <- readRDS("../data/hamby224_lime.rds")
  
}
```

```{r}
# Apply explain function if the file does not already exist
if(!file.exists("../data/hamby224_explain.rds")) {
  
  # Apply the explain function from the lime package (with a seed set)
  set.seed(84902)
  hamby224_explain <- lime::explain(hamby224_test %>% 
                                      arrange(case) %>% 
                                      select(rf_features) %>% 
                                      na.omit(), 
                                    hamby224_lime,
                                    labels = "FALSE",
                                    n_features = 3)
  
  # Save the explainer object
  saveRDS(hamby224_explain, "../data/hamby224_explain.rds")
  
} else {
  
  # Load in the explain object
  hamby224_explain <- readRDS("../data/hamby224_explain.rds")
  
}
```

```{r}
if (!file.exists("../data/hamby_bins.csv")) {
  
  # Create a dataframe of bins cuts
  hamby_bins <- data.frame(matrix(unlist(hamby224_lime$bin_cuts), 
                                  nrow = 9, byrow = TRUE)) %>%
      rename("Q0" = X1, "Q25" = X2, "Q50" = X3, "Q75" = X4, "Q100" = X5) %>%
      mutate(Feature = rf_features) %>%
      select(Feature, Q0:Q100)
  
  # Save the bin cuts
  write.csv(hamby_bins, "../data/hamby_bins.csv", row.names = FALSE)
  
} else {
  
  # Load in the bins csv
  hamby_bins <- read.csv("../data/hamby_bins.csv")
  
}
```

```{r}
# Function to use for creating bin labels in the test_explain dataset
bin_labeller <- function(feature, feature_value, bin_cuts = hamby_bins){

  if (is.na(feature)) {
    
    # Set feature_bin to NA if feature is NA
    feature_bin <- NA
    
  } else {
    
    # Subset the bin cuts table to the selected feature
    feature_bin_cuts <- bin_cuts %>% filter(Feature == feature)
  
    # Determine which bin the case falls in
    if(feature_value <= feature_bin_cuts$Q25){
      feature_bin <- paste(feature, "(lower bin)")
    } else if (feature_bin_cuts$Q25 < feature_value & 
               feature_value <= feature_bin_cuts$Q50){
      feature_bin <- paste(feature, "(lower middle bin)")
    } else if (feature_bin_cuts$Q50 < feature_value & 
               feature_value <= feature_bin_cuts$Q75){
      feature_bin <- paste(feature, "(upper middle bin)")
    } else if (feature_bin_cuts$Q75 < feature_value){
      feature_bin <- paste(feature, "(upper bin)")
    }
       
  }

  # Return the bin
  return(feature_bin)

}
```

```{r}
# Create the test_explain combined data if the file does not already exist
if(!file.exists("../data/hamby224_test_explain.rds")) {
  
  # Join the data and the explanations and edit and add additional variables
  hamby224_test_explain <- hamby224_test %>%
    mutate(case = as.character(case)) %>%
    full_join(hamby224_explain, by = "case") %>%
    mutate(case = factor(case),
           feature_desc = factor(feature_desc),
           feature = factor(feature),
           feature_bin = mapply(bin_labeller, feature = feature, 
                                feature_value = feature_value)) %>%
    mutate(feature_number = readr::parse_number(feature_desc),
           strictly_less = FALSE)
  
  # Finish creating the strictly less than variable
  hamby224_test_explain$strictly_less[grep("< ", 
                                           hamby224_test_explain$feature_desc)] <- TRUE
  
  # Reorder the variables of feature_desc and feature_bin for plotting purposes
  hamby224_test_explain <- hamby224_test_explain %>%
    mutate(feature_desc = reorder(feature_desc, strictly_less),
           feature_desc = reorder(feature_desc, feature_number),
           feature_desc = reorder(feature_desc, as.numeric(feature))) %>%
    select(case:feature_desc, feature_bin:strictly_less, data, prediction)

  # Save the combined test and explain data
  saveRDS(hamby224_test_explain, "../data/hamby224_test_explain.rds")
   
} else {
  
  # Load in the data
  hamby224_test_explain <- readRDS("../data/hamby224_test_explain.rds")
  
}
```

```{r}
# Check to see if model_prediction is the sum of the coefficients from the simple model
hamby224_test_explain %>% 
  filter(case == 1) %>%
  select(model_intercept, feature, feature_weight, model_prediction, rfscore)

simple_model <-  data.frame(term = c("intercept", 
                    as.character(hamby224_test_explain %>% filter(case == 1) %>% pull(feature))),
           coefficients = c(subset(hamby224_test_explain, case == 1)$model_intercept[1],
                            hamby224_test_explain %>% filter(case == 1) %>% pull(feature_weight)))

# It is!
sum(simple_model$coefficients)
subset(hamby224_test_explain, case == 1)$model_prediction[1]
```

```{r}
check <- hamby224_test_explain %>%
  select(-data, -prediction) %>%
  group_by(case) %>%
  slice(1) %>%
  select(case, set, rfscore, model_prediction, model_r2) %>%
  mutate(diff = rfscore - model_prediction,
         mean = (rfscore + model_prediction) / 2)

sum((subset(check$diff, check$set == "Set 1"))^2, na.rm = TRUE)
sum((subset(check$diff, check$set == "Set 11"))^2, na.rm = TRUE)
```

Below is are plots of the probability of a match computed from the "simple" (ridge regression) model in `lime` versus the probability of a match computed from the `rtrees` random forest model for both the Hamby224 Set 1 and Set 11 test cases. Linear regression lines have been fit through the points. Both plots show that there is a weak negative linear relationship between the two variables. This suggests that as the probability of a match increases in the random forest, the simple model is doing a worse job of computing a probability that agrees with the random forest model. However, the Hamby 224 test sets 1 and 11 do not have many observations with high random forest scores. Each test set should only have 18 matches. The trend may be different if there was more cases with high random forest scores. However, it appears that the cases that are a match have some of the lowest model_prediction scores overall.

THe points are also colored by the $R^2$ value from the simple model. This plot suggests that the simple models with a prediction close to 0.5 have a better fit, but the $R^2$ values are still low. I am not sure what to make of this. However, it does not appear that the $R^2$ is related to the probability from the random forest.

```{r fig.width = 8, fig.height = 4}
ggplot(check, aes(x = rfscore, y = model_prediction)) + 
  geom_point(aes(color = model_r2)) + 
  facet_grid( ~ set) + 
  theme_bw() + 
  stat_smooth(method = "lm")
```

The plots below show the absolute value of the difference between the random forest probability and the simple model probabilty versus the random forest probability. These plots show a clear v-shaped trend that suggests that the simple model is doing the best job of replicating the random forest probabilty around the random forest probability of 0.5. That is, the simple model seems to be doing a worse job of computing accurate predictions in the extremes of the random forest scores.

```{r fig.width = 8, fig.height = 4}
ggplot(check, aes(x = rfscore, y = abs(diff))) + 
  geom_point(aes(color = model_r2)) + 
  facet_grid( ~ set) +
  theme_bw()
```

The plot below shows the difference of the two model probabilities versus the random forest score. This shows that when the random forest score is low, the simple model is over estimating the probability, and when the random forest score is high, the simple model is underestimating the probability.

```{r fig.width = 8, fig.height = 4}
ggplot(check, aes(x = rfscore, y = diff)) + 
  geom_point(aes(color = model_r2)) + 
  facet_grid( ~ set) +
  geom_hline(yintercept = 0, color = "blue") +
  theme_bw()
```

## Visualizing the LIME Explanations

```{r}
hamby224_test_explain %>%
  ggplot(aes(x = feature_desc)) + 
  geom_bar() +
  coord_flip()
```

```{r}
hamby224_test_explain %>%
      filter(set == "Set 1") %>%
      mutate(rfscore = round(rfscore, 3)) %>%
      select(case, bullet1, bullet2, land1, land2, rfscore) %>%
      distinct() %>%
      ggplot(aes(x = land1, y = land2, label = bullet1, label2 = bullet2,
                 text = paste('Bullets Compared: ', bullet1, "-", land1, 
                              "vs", bullet2, "-", land2,
                              '\nRandom Forest Score: ', 
                              ifelse(is.na(rfscore), "Missing due to tank rash", rfscore)))) +
      geom_tile(aes(fill = rfscore)) +
      facet_grid(bullet2 ~ bullet1, scales = "free") +
      theme_minimal() +
      scale_fill_gradient2(low = "darkgrey", high = "darkorange", midpoint = 0.5) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      labs(x = "", y = "", fill = "RF Score")
```

# Results

# Discussion

# References

