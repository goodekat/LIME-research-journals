---
title: "Interpreting Random Forest Predictions for Bullet Matching Using LIME (?)"
author: "Katherine Goode and Heike Hofmann"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: true
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

# Load libraries
library(dplyr)
library(ggplot2)
library(lime)
```

# Introduction

- background
- motivation
- literature review
- problem statement

Bullet matching is a practice that has historically been performed by hand by specially trained firearm examiners. The examiners visually assess the striation markings on the bullets under a high powered microscope to determine whether the bullets were fired from the same gun barrel. In more recent times, the scientific community has been calling for the inclusion of more data driven techniques to be used in forensic investigations that would allow for the reporting of an uncertainty along with the conclusion drawn from the analysis. 

This led Hare, Hofmann, and Carriquiry (2017) to propose a new computer automated method of bullet matching. The method involves obtaining a bullet signature from a scan of the bullet, computing a handful of variables that measure the similarity between two signatures, and fitting a random forest model to the obtained similarity variables. The model can be trained on a set of known bullet matches and non-matches and used to predict on a new set of bullet signature comparisons. The results from the paper suggest that the random forest model leads to accurate bullet matching predictions. The authors found that when their model was applied to a testing dataset (?), the resulting error rate was 0%.

While random forest models often result in good predictions (reference?), it well known that a disadvantage of random forest models and other machine learning techniques is that it is difficult to understand which variables played an important role in the creation of the predictions. This issue led to the development of LIME (reference), which is an algorithm that examines the behavior of the complicated model on a local scale around a new prediction.

Bullet matching is commonly used as evidence for convictions in court cases. Thus, it is important to not only provide accurate predictions and an associated uncertainty level but also to explain which variables were important in making the prediction. This paper applies LIME to the random forest model used for bullet matching to assess which variables are driving the predictions made by the model.

# Data

## Training Data: The Hamby...

(fill in once we know which data will be used to train the new random forest model)

## Testing Data: The Hamby 224 Clone

The Hamby 224 Clone is organized as a test set of a cloned (sub-)set of the Hamby 224 bullets. As with all Hamby sets [@hamby], Hamby set 224, is a collection of 35 bullets, organized as 20 known bullets and 15 questioned bullets. The known bullets are fired in pairs of two through one of ten consecutively manufactures P-85 barrels.
Clone set 224 is arranged as a test set of fifteen tests, one for each questioned bullet. Each test set is arranged as a combination of three bullets: two known bullets and a questioned bullet. The test asks for a decision on whether the questioned bullet comes from the same source as the two known bullets  or from a different source. This situation is similar to what a Firearms and Toolmarks Examiner might encounter in case work. 

#  Methods

## Random Forest Model

(fill in once the new random forest model has been fit)

## Overview of LIME

## Applying LIME

## Visualizing the LIME Explanations

```{r}
# Read in the Hamby 224 combined test data and explanations
hamby224_test_explain <- readRDS("../data/hamby224_test_explain.rds")
```

```{r}
hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, rfscore) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = rfscore)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw() +
  scale_fill_gradient2(low = "grey", high = "orange", midpoint = 0.5)

hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, mypred) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = mypred)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw() +
  scale_fill_gradient2(low = "grey", high = "orange", midpoint = 0.5)

hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, label_prob) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = label_prob)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw() +
  scale_fill_gradient2(low = "grey", high = "orange", midpoint = 0.5)

hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, samesource) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = samesource)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw()
```

```{r}
hamby224_explain <- readRDS("../data/hamby224_explain.rds")

lime::plot_explanations(hamby224_explain)

hamby224_test_explain %>%
  filter(set == "1") %>%
  ggplot(aes(x = case, y = feature_desc)) + 
  geom_tile(aes(fill = feature_weight)) + 
  facet_grid(feature ~ label, space = "free", scale = "free") + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_gradient2(midpoint = 0)

hamby224_test_explain %>%
  filter(set == "1", samesource == "FALSE") %>%
  ggplot(aes(x = case, y = feature_desc)) + 
  geom_tile(aes(fill = feature_weight)) + 
  facet_grid(feature ~ bullet1, space = "free", scale = "free") + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_gradient2(midpoint = 0)

hamby224_test_explain %>%
  filter(set == "1", samesource == "FALSE") %>%
  ggplot(aes(x = case, y = feature_desc)) + 
  geom_tile(aes(fill = feature_weight)) + 
  facet_grid(bullet1 ~ bullet2, space = "free", scale = "free") + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_gradient2(midpoint = 0)
```

# Results

# Discussion

# References

