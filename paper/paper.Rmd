---
title: "Interpreting Random Forest Predictions for Bullet Matching Using LIME (?)"
author: "Katherine Goode and Heike Hofmann"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: true
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, cache = TRUE)

# Load libraries
library(tidyverse)
library(bulletr)
library(lime)
library(randomForest)
```

# Introduction

- background
- motivation
- literature review
- problem statement

Bullet matching is a practice that has historically been performed by hand by specially trained firearm examiners. The examiners visually assess the striation markings on the bullets under a high powered microscope to determine whether the bullets were fired from the same gun barrel. In more recent times, the scientific community has been calling for the inclusion of more data driven techniques to be used in forensic investigations that would allow for the reporting of an uncertainty along with the conclusion drawn from the analysis. 

This led Hare, Hofmann, and Carriquiry (2017) to propose a new computer automated method of bullet matching. The method involves obtaining a bullet signature from a scan of the bullet, computing a handful of variables that measure the similarity between two signatures, and fitting a random forest model to the obtained similarity variables. The model can be trained on a set of known bullet matches and non-matches and used to predict on a new set of bullet signature comparisons. The results from the paper suggest that the random forest model leads to accurate bullet matching predictions. The authors found that when their model was applied to a testing dataset (?), the resulting error rate was 0%.

While random forest models often result in good predictions (reference?), it well known that a disadvantage of random forest models and other machine learning techniques is that it is difficult to understand which variables played an important role in the creation of the predictions. This issue led to the development of LIME (reference), which is an algorithm that examines the behavior of the complicated model on a local scale around a new prediction.

Bullet matching is commonly used as evidence for convictions in court cases. Thus, it is important to not only provide accurate predictions and an associated uncertainty level but also to explain which variables were important in making the prediction. This paper applies LIME to the random forest model used for bullet matching to assess which variables are driving the predictions made by the model.

# Data

## Training Data: The Hamby...

(fill in once we know which data will be used to train the new random forest model)

## Testing Data: The Hamby 224 Clone

The Hamby 224 Clone is organized as a test set of a cloned (sub-)set of the Hamby 224 bullets. As with all Hamby sets [@hamby], Hamby set 224, is a collection of 35 bullets, organized as 20 known bullets and 15 questioned bullets. The known bullets are fired in pairs of two through one of ten consecutively manufactures P-85 barrels.
Clone set 224 is arranged as a test set of fifteen tests, one for each questioned bullet. Each test set is arranged as a combination of three bullets: two known bullets and a questioned bullet. The test asks for a decision on whether the questioned bullet comes from the same source as the two known bullets  or from a different source. This situation is similar to what a Firearms and Toolmarks Examiner might encounter in case work. 

#  Methods

```{r echo = FALSE}

# Cleaning the Hamby 224 data -------------------------------------------------

# Load in the Hamby 224 datasets
hamby224_set1 <- readRDS("../data/h224-set1-features.rds")
hamby224_set11 <- readRDS("../data/h224-set11-features.rds")

# Obtain features used when fitting the rtrees random forest
rf_features <- rownames(rtrees$importance)

# Clean the Hamby 224 set 1 data
hamby224_set1_cleaned <- hamby224_set1 %>%
  select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
  rename(bullet1 = bulletA,
         bullet2 = bulletB, 
         land1 = landA,
         land2 = landB) %>%
  mutate(study = factor("Hamby_224"), 
         set = factor("1"),
         bullet1 = factor(bullet1),
         bullet2 = factor(bullet2),
         land1 = factor(land1),
         land2 = factor(land2)) %>%
  select(study, set, bullet1:land2, rf_features, rfscore, samesource)

# Clean the Hamby 224 set 11 data
hamby224_set11_cleaned <- hamby224_set11 %>%
  select(-bullet_score, -land1, -land2, -aligned, -striae, -features) %>%
  rename(bullet1 = bulletA,
         bullet2 = bulletB, 
         land1 = landA,
         land2 = landB) %>%
  mutate(study = factor("Hamby_224"), 
         set = factor("2"),
         bullet1 = recode(factor(bullet1), 
                          "Bullet 1" = "1", 
                          "Bullet 2" = "2", 
                          "Bullet I" = "I"),
         bullet2 = recode(factor(bullet2), 
                          "Bullet 1" = "1", 
                          "Bullet 2" = "2", 
                          "Bullet I" = "I"),
         land1 = recode(factor(land1), 
                        "Land 1" = "1", "Land 2" = "2", "Land 3" = "3", 
                        "Land 4" = "4", "Land 5" = "5", "Land 6" = "6"),
         land2 = recode(factor(land2), 
                        "Land 1" = "1", "Land 2" = "2", "Land 3" = "3", 
                        "Land 4" = "4", "Land 5" = "5", "Land 6" = "6")) %>%
  select(study, set, bullet1:land2, rf_features, rfscore, samesource)

# Join the two cleaned Hamby 224 sets into one testing set
hamby224_test <- suppressWarnings(bind_rows(hamby224_set1_cleaned,
                                            hamby224_set11_cleaned))
```

## Random Forest Model

(fill in once the new random forest model has been fit)

## Overview of LIME

## Applying LIME

```{r echo = FALSE}

# Applying LIME -------------------------------------------------

# Load in the training data (Hamby Data 173 and 252)
hamby173and252_train <- read.csv("../data/hamby173and252_train.csv")

# Apply the lime function from the lime package (with a seed set)
# Note that the as_classifier must be added since rtrees is from the
# randomForest package and not fit using caret or one of the other 
# available models specified in the lime package. Additionally, the
# randomForest package must be loaded in order to run the functions
# from the lime package. (I should check on exactly how this works.)
set.seed(84902)
hamby224_lime <- lime(x = hamby173and252_train %>% select(rf_features),
                      model = as_classifier(rtrees))

# Apply the explain function from the lime package (with a seed set)
set.seed(84902)
hamby224_explain <- lime::explain(hamby224_test %>% select(rf_features), 
                                  hamby224_lime, 
                                  n_labels = 1, 
                                  n_features = 3)

# Add a case variable to the test data
hamby224_test <- hamby224_test %>%
  mutate(case = as.character(1:dim(hamby224_test)[1])) %>%
  select(case, study:samesource)

# Obtain features used when fitting the rtrees random forest
rf_features <- rownames(rtrees$importance)

# Join the data and the explanations and edit and add additional variables
hamby224_test_explain <- full_join(hamby224_test, hamby224_explain, by = "case") %>%
  mutate(case = factor(case),
         set = factor(set),
         bullet1 = factor(bullet1),
         bullet2 = factor(bullet2),
         land1 = factor(land1),
         land2 = factor(land2)) %>%
  mutate(mypred = rep(predict(rtrees, hamby224_test %>% select(rf_features), 
                              type = "prob")[,2], each = 3)) %>%
  select(case:rfscore, mypred, samesource:prediction)

hamby224_test_explain$feature_desc <- factor(hamby224_test_explain$feature_desc)
hamby224_test_explain$feature <- factor(hamby224_test_explain$feature)

hamby224_test_explain$feature_number <- readr::parse_number(hamby224_test_explain$feature_desc)
hamby224_test_explain$strictly_less <- FALSE
hamby224_test_explain$strictly_less[grep("< ", hamby224_test_explain$feature_desc)] <- TRUE

hamby224_test_explain <- hamby224_test_explain %>%
  mutate(
    feature_desc = reorder(feature_desc, strictly_less),
    feature_desc = reorder(feature_desc, feature_number),
    feature_desc = reorder(feature_desc, as.numeric(feature))
  )
```

## Visualizing the LIME Explanations

Some plots to tryout working with the new explanations...

```{r}
hamby224_test_explain %>%
  ggplot(aes(x = feature_desc)) + 
  geom_bar() +
  coord_flip()

hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, rfscore) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = rfscore)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw() +
  scale_fill_gradient2(low = "grey", high = "orange", midpoint = 0.5)

hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, mypred) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = mypred)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw() +
  scale_fill_gradient2(low = "grey", high = "orange", midpoint = 0.5)

hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, label_prob) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = label_prob)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw() +
  scale_fill_gradient2(low = "grey", high = "orange", midpoint = 0.5)

hamby224_test_explain %>%
  filter(set == "1") %>%
  select(case, bullet1:land2, samesource) %>%
  distinct() %>%
  ggplot(aes(x = land1, y = land2)) +
  geom_tile(aes(fill = samesource)) +
  facet_grid(bullet1 ~ bullet2) +
  theme_bw()
```

```{r, fig.width = 8, fig.height = 4}
hamby224_explain <- readRDS("../data/hamby224_explain.rds")

lime::plot_explanations(hamby224_explain)

hamby224_test_explain %>%
  filter(set == "1") %>%
  ggplot(aes(x = case, y = feature_desc)) + 
  geom_tile(aes(fill = feature_weight)) + 
  facet_grid(feature ~ label, space = "free", scale = "free") + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_gradient2(midpoint = 0) +
  theme(legend.position = "bottom") +
  ylab("Feature Sets") 
  

hamby224_test_explain %>%
  filter(set == "1", samesource == "FALSE") %>%
  ggplot(aes(x = case, y = feature_desc)) + 
  geom_tile(aes(fill = feature_weight)) + 
  facet_grid(feature ~ bullet1, space = "free", scale = "free") + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_gradient2(midpoint = 0)

hamby224_test_explain %>%
  filter(set == "1", samesource == "FALSE") %>%
  ggplot(aes(x = case, y = feature_desc)) + 
  geom_tile(aes(fill = feature_weight)) + 
  facet_grid(bullet1 ~ bullet2, space = "free", scale = "free") + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_gradient2(midpoint = 0) +
  theme(legend.position = "bottom")
```

# Results

# Discussion

# References

