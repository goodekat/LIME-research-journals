\documentclass[AMS,STIX1COL]{WileyNJD-v2}

%\usepackage{amsmath,amsfonts}

\articletype{Article Type}%

\received{26 April 2016}
\revised{6 June 2016}
\accepted{6 June 2016}

\raggedbottom

\usepackage{Sweave}
\begin{document}
\input{visual-diagnostics-of-a-model-explainer-concordance}

\title{Visual Diagnostics of a Model Explainer -- Tools for the Assessment of LIME Explanations}

\author[1]{Katherine Goode*}

\author[1,2]{Heike Hofmann}

\authormark{Goode and Hofmann}

\address[1]{\orgdiv{Department of Statistics}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}

\address[2]{\orgdiv{Center for (CSAFE}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}

\corres{*Corresponding author. \email{kgoode@iastate.edus}}

\presentaddress{This is sample for present address text this is sample for present address text}

\abstract[Summary]{This is sample abstract text.}

\keywords{LIME, black box models, interpretability, diagnostics}

\jnlcitation{\cname{%
\author{Goode K.}, 
\author{H. Hofmann}, 
} (\cyear{2016}), 
\ctitle{A regime analysis of Atlantic winter jet variability applied to evaluate HadGEM3-GC2}, \cjournal{Q.J.R. Meteorol. Soc.}, \cvol{2017;00:1--6}.}

\maketitle

\section{Introduction}

Machine learning algorithms have been proven to provide accurate predictions. However, this accuracy comes at a cost. With complicated algorithms, the ability to understand the mechanism in the data and the functional form of the model are lost. This has led for a desire to be able to interpret machine learning algorithms. One approach to remedy this problem would be to develop a model explainer. The objective of such an explainer is to understand and explain model performance.

Locally interpretable model explanations (LIME) is one such model explainer that was developed for this purpose (reference). While some model explainers are focused on understanding a model at the global level, LIME was designed to explain the model for a single prediction of interest. Conceptually, LIME fits a simple interpretable model (the explainer) that is meant to capture the behavior of the complex "black box model" in a local region around the prediction. 

Predictive models are used in both regression and classification settings. For this paper, we will focus on the classification setting, and in particular, we will only discuss the case with a dichotomous response variable. In any case, when a predictive model is being developed, the data is usually divided into training and testing portions. The training data is used to fit the complex model, and the testing data is used to assess the model. The complex model can applied to the features in the testing data, and the resulting predictions can be compared to ground truth. Of interest during this assessment are the cases when the model is wrong. In the dichotomous response classification case, there are two ways in which the model can be wrong. The model can make a type I error in which..., or the model can make a type II error in which...

It does this by using the features from the training data to simulate a new dataset on which the simple model is fit. The complex model is applied to the simulated dataset to obtain predictions. The observations associated with predictions are used as the response variable in a ridge regression model with the the simulated features as the predictor variable with the highest weight given to observations closest to the prediction of interest. Feature selection is performed to identify the most important variables in the local region. A final ridge regression model is fit with the selected features, and the coefficients of the model are used to interpret the behavior of the complex model.

- explanation is also a prediction
  - how reliable is that explanation?
- we will present some tools that could help answer that question
- we will show an example using this tools (bullet data)
- we will mention some possible insights about LIME that these tools have shown us
1. explainer model generally has very low $R^2$ (probably due to binning)
2. "local" explanations are not local but are driven by the ("global") marginal distributions of covariates 
- Describe LIME including details on binning and linear regression in binned features and motivation for the binning

\section{Data}

\section{Methods}

\subsection{LIME Algorithm}

\paragraph{Notation}

\begin{itemize}
\item $x\in\mathbb{R}^d$: original representation of an instance being explained
\begin{itemize}
\item e.g. feature vector containing word embeddings
\end{itemize}
\item $x'\in\mathbb{R}^{d'}$: vector for the interpretable representation of the instance being explained
\begin{itemize}
\item e.g. bag of words
\end{itemize}
\item $G$: class of potentially interpretable models
\begin{itemize}
\item e.g. linear models, decision trees, rule lists
\end{itemize}
\item $g$: explanation model where $g:\mathbb{R}^{d'}\rightarrow\mathbb{R}$ and $g\in G$
\item $\Omega(g)$: measure of complexity of $g$
\begin{itemize}
\item e.g. depth of a tree, number of non-zeros in a linear model
\end{itemize}
\item $f$: model that is being explained where $f:\mathbb{R}^d\rightarrow\mathbb{R}$
\begin{itemize}
\item note: in classification $f(x)$ is the probability that $x$ belongs to a certain class
\end{itemize}
\item $\Pi_x(z)$: proximity measure between an instance $z$ to $x$ which defines a locality around $x$
\item $\mathcal{L}(f, g, \Pi_x)$: a measure of how unfaithful $g$ is in approximating $f$ in the locality defined by $\Pi_x$
\end{itemize}

\paragraph{Step by Step Procedure}

Step 1: 

\subsection{Application of LIME to Bullet Matching Data}

\subsubsection{lime R Package Perturbation Creation Methods}

The LIME R package allows for the following four methods to sample the perturbations based on the distributions of the features from the training data.

\begin{itemize}
\item Equally Spaced Bins
\item Quantile Bins
\item Normal Approximation
\item Kernel Density Approximation
\end{itemize}

The methods of equally spaced bins and quantile bins also allow the user to specify the number of bins. As of now, there are no recommendations or procedures provided for how to determine which method to use. By default, LIME uses four quantile bins. It was of interest to see how the explanations from LIME varied across the four sampling methods when applied to the bullet matching data. The LIME algorithm was applied to each prediction from the test data obtained from the 'rtrees' random forest model for each of the four sampling methods. Within the bin based sampling methods, the algorithm was applied for 2 to 6 bins. It was decided to only go up to 6 bins since the more bins used the more complex the explanation becomes.

\subsubsection{Proposed Bin Creation Methods}

\section{Results}

\subsection{LIME Package Explanations Dependent on Sampling Method}

In order to assess the LIME explanations created using different sampling methods, it was of interest to compare the top three features chosen as the important predictors by lime within a case from the test data across the diferent sampling methods. Figure ... is a heatamp showing the top feature chosen by lime for each of the cases in the test data and different bin based sampling methods. The rows represent the cases in the test data, and the columns represent the sampling methods. There are twenty methods included in the plot. These include the equally spaced bins and quantile bins from the lime package and the random forest score tree based bins and the same source tree based bins proposed in this paper. The rows are facetted by the test set and whether or not the observation is a match or not. The columns are faceted by these methods, and the columns within a facet represent the number of bins. Each method has 2 to 6 bins. The colors represent the top feature chosen by lime.

The variables of ccf and cms immediately show up as common variables chosen across all of the sampling methods. However, the patterns across the number of bins withing the sampling methods are different. When equally spaced bins are used, the top feature chosen is consistent across all cases within a number of bins category. For example, ccf is almost always chosen (change to actual number) with 2 equally spaced bins, matches is almost always chosen with 3 equally spaced bins, and non\_cms is always chosen for the nonmatches with 5 and 6 equally spaced bins. This shows that with the bullet matching data, the top feature chosen with equally spaced bins is an artifact of the number of bins used. With equally spaced bins, this figure sugggest that LIME is providing global explanations as opposed to local explanations. It would be preferable that the top feature chosen was more consistent across the number of bins and more variable across the cases. This would suggest that the top feature chosen is dependent on the feature values associated with a particular case and not just on which feature is the best explainer when $b$ number of bins are used.

%\backmatter

\section*{Acknowledgments}
This is acknowledgment text~\cite{Elbaum2002}. Provide text here. This is acknowledgment text. Provide text here. This is acknowledgment text. Provide text here. This is acknowledgment text. Provide text here. This is acknowledgment text. Provide text here. This is acknowledgment text. Provide text here. This is acknowledgment text. Provide text here. This is acknowledgment text. Provide text here. This is acknowledgment text. Provide text here. 

\subsection*{Author contributions}

This is an author contribution text. This is an author contribution text. This is an author contribution text. This is an author contribution text. This is an author contribution text. 

\subsection*{Financial disclosure}

None reported.

\subsection*{Conflict of interest}

The authors declare no potential conflict of interests.


\section*{Supporting information}

The following supporting information is available as part of the online article:

\noindent
\textbf{Figure S1.}
{500{\uns}hPa geopotential anomalies for GC2C calculated against the ERA Interim reanalysis. The period is 1989--2008.}

\noindent
\textbf{Figure S2.}
{The SST anomalies for GC2C calculated against the observations (OIsst).}


\appendix

\section{Section title of first appendix\label{app1}}

%\nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;
\bibliography{wileyNJD-AMS}%

\section*{Author Biography}

\begin{biography}{\includegraphics[width=60pt,height=70pt,draft]{empty}}{\textbf{Author Name.} This is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text.}
\end{biography}

\end{document}
