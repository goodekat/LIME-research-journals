<!DOCTYPE html>
<html>
  <head>
    <title> An Application of LIME to a Random Forest</title>
    <meta charset="utf-8">
    <meta name="author" content=" Katherine Goode" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <a href="https://en.wikipedia.org/wiki/Tilia"><img src="images/lindens.jpg" /></a> An Application of LIME to a Random Forest
### <font size="5"> Katherine Goode </font>
### <font size="5"> ISU Graphics Group, March 1, 2019 </font>

---




# Overview

.pull-left[
### Goals for the talk
- Explain the LIME algorithm
- Present an application of LIME
- Highlight problems with this application

### The plan

1. Explanation of LIME 
2. Hamby bullet data
3. Applying LIME to the bullet data
4. Issues with LIME
5. Attempts at a solution
6. Future work
]

.pull-right[
&lt;br&gt;
&lt;br&gt;
&lt;img src="./images/lime_drawing.png" width = 400&gt;
]

---

class: inverse, center, middle

# What is &lt;span style="color:lime"&gt;LIME&lt;/span&gt;?

---

# Motivation for LIME

&lt;br&gt;

[![](./images/blackbox.png)](https://en.wikipedia.org/wiki/Black_box)

### Black Box Prediction Models

- Offer great predictive ability 
- Loss of interpretability
- Difficult to assess trustworthiness

### Enter LIME

- **L**ocal **I**nterpretable **M**odel-Agnostic **E**xplanations
- Developed by computer scientists ([Ribeiro, Singh, and Guestrin](https://arxiv.org/pdf/1602.04938.pdf))
- Designed to assess if a black box predictive model is trustworthy
- Produces "explanations" for individual predictions

---

# Meaning of LIME

.pull-left[
### &lt;span style="color:lime"&gt;L&lt;/span&gt;ocal

- Focuses on behavior of a complex model at a local level

### &lt;span style="color:lime"&gt;I&lt;/span&gt;nterpretable

- Produces easily interpretable "explanations"

### &lt;span style="color:lime"&gt;M&lt;/span&gt;odel-Agnostic

- Works with any black-box predictor

### &lt;span style="color:lime"&gt;E&lt;/span&gt;xplanations

- Provides insight into individual predictions
]

.pull-right[
&lt;br&gt;
&lt;br&gt;
.center[&lt;img src="./images/local.png" width=300&gt;]
.center[&lt;font size="4"&gt;Figure 3 in Ribeiro, Singh, and Guestrin (2016)&lt;/font&gt;]
&lt;br&gt;
&lt;br&gt;
.center[&lt;img src="./images/image_explanation.png" width=500&gt;]
.center[&lt;font size="4"&gt;Figure 4 in Ribeiro, Singh, and Guestrin (2016)&lt;/font&gt;]
]

---

# An Example &lt;font size="3"&gt;(from Ribeiro, Singh, and Guestrin (2016))&lt;/font&gt;

.pull-left[
### 1. Black Box Model 
- Model predicts whether a patient has the flu
- Apply the model to a new patient
- Predicts that the patient has the flu
- Can this prediction be trusted?
]

.pull-right[
### 2. LIME
- Apply LIME to *this* case
- LIME returns the most important variables in *this* prediction
- Colors indicate
    + &lt;span style="color:green"&gt;green&lt;/span&gt;: evidence supporting the flu
    + &lt;span style="color:red"&gt;red&lt;/span&gt;: evidence against the flu
- Can this prediction be trusted?
]

.center[&lt;img src="./images/example.png" width=700&gt;]
.center[&lt;font size="4"&gt;Figure 1 in Ribeiro, Singh, and Guestrin (2016)&lt;/font&gt;]
 
---

# Idea of the LIME Procedure

Start with training data, complex model, and one prediction from the test data
1. Use the features in the training data to create a new dataset of many perturbations.
2. Obtain a prediction for each perturbation using the complex model.
3. Compute a similarity score between the test observation and each perturbation. 
4. Perform feature selection to choose `\(k\)`.
5. Fit a linear regression (weighted by the similarity scores) to the perturbations with the `\(k\)` chosen features (standardized).
  `$$\mbox{prediction} \sim \mbox{chosen feature 1} + \cdots + \mbox{chosen feature k}$$`
6. Use the coefficients from the regression to "explain" the predictions.

| | Feature 1 | &lt;span style="color:blue"&gt;Feature 2&lt;/span&gt; | &lt;span style="color:blue"&gt;Feature 3&lt;/span&gt; | Feature 4 | Similarity Score | Prediction |
| --: | :--: | :--: | :--: | :--: | :--: | :--: |
| test observation | 2 | 4 | 2 | 10 | exact | `\(\hat{y}_{\mbox{obs}}\)` |
| perturbation 1 | 2 | 3 | 2 | 12 | very close | `\(\hat{y}_1\)` |
| `\(\vdots\)` | `\(\vdots\)` | ... | ... | ... | ... | 
| perturbation 5000  | 10 | 0 | 13 | 22 | not close | `\(\hat{y}_{5000}\)` |

---

# Using LIME

### Implementations of LIME

- Developers created a Python package called [lime](https://github.com/marcotcr/lime)
- Thomas Pedersen created an R package also called [lime](https://github.com/thomasp85/lime)

.pull-left[

```r
#install.packages("lime")
library(lime)
?lime
?explain
```
]

.pull-right[![](./images/lime.png)]
---

class: inverse, center, middle

# Hamby Bullet Matching Data

---
class: inverse, center, middle

# Applying LIME to the bullet data

---

# &lt;span style="color:lime"&gt;LIME&lt;/span&gt; Procedure with Iris Data (in R)

## Step 0: Split iris data into training and testing datasets


```r
# Iris dataset
iris[1:3, ]
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
```

```r
# Split up the data set into training and testing datasets
iris_test &lt;- iris[1:5, 1:4]
iris_train &lt;- iris[-(1:5), 1:4]

# Create a vector with the responses for the training dataset
iris_lab &lt;- iris[[5]][-(1:5)]
```

---

## Step 1: Fit a complex model to the training data


```r
# Create random forest model on iris data
library(caret)
```

```
## Loading required package: lattice
```

```
## Loading required package: ggplot2
```

```r
rf_model &lt;- train(iris_train, iris_lab, method = 'rf')

# Can use the complex model to make predictions
Pred &lt;- predict(rf_model, iris_test)
Actual &lt;- iris[1:5, 5]
data.frame(iris_test, Pred, Actual)
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width   Pred Actual
## 1          5.1         3.5          1.4         0.2 setosa setosa
## 2          4.9         3.0          1.4         0.2 setosa setosa
## 3          4.7         3.2          1.3         0.2 setosa setosa
## 4          4.6         3.1          1.5         0.2 setosa setosa
## 5          5.0         3.6          1.4         0.2 setosa setosa
```

---

## Step 2: Obtain distributions of the variables from the training data


```r
# Create an explainer object
library(lime); explainer &lt;- lime(iris_train, rf_model)

# Sepal length quantiles obtained from training data
explainer$bin_cuts$Sepal.Length
```

```
##   0%  25%  50%  75% 100% 
##  4.3  5.2  5.8  6.4  7.9
```

```r
# Probability distribution for sepal length
explainer$feature_distribution$Sepal.Length
```

```
## 
##         1         2         3         4 
## 0.2758621 0.2413793 0.2413793 0.2413793
```

---

## | Histogram of Sepal Length from Training Data


```r
# Histogram of sepal length from training data
ggplot(iris_train, aes(x = Sepal.Length)) +
  geom_histogram(bins = 10, color = "grey") +
  geom_vline(xintercept = explainer$bin_cuts[[1]][[1]], color = "green", size = 1) +
  geom_vline(xintercept = explainer$bin_cuts[[1]][[2]], color = "green", size = 1) +
  geom_vline(xintercept = explainer$bin_cuts[[1]][[3]], color = "green", size = 1) +
  geom_vline(xintercept = explainer$bin_cuts[[1]][[4]], color = "green", size = 1) +
  geom_vline(xintercept = explainer$bin_cuts[[1]][[5]], color = "green", size = 1) +
  labs(x = "Sepal Length", y = "Count")
```

&lt;img src="An_Application_of_LIME_to_a_Random_Forest_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

## Step 3: Sample from each of the variable distributions `\(n\)` times

Histograms of predictor variables from training data


```r
knitr::include_graphics("./Graphics/training_distributions.png")
```

&lt;img src="./Graphics/training_distributions.png" width="500px" style="display: block; margin: auto;" /&gt;

---

## | Histograms of `\(n=5000\)` samples from distributions of training variables for each of the five testing cases


```r
knitr::include_graphics("./Graphics/samples.png")
```

&lt;img src="./Graphics/samples.png" width="550px" style="display: block; margin: auto;" /&gt;

---

## Step 4: Obtain predictions for sampled values using the complex model

* For each testing case use the random forest model to make a prediction for each of the `\(n=5000\)` samples
* In the iris data, the predictions are represented by the probability that a flower is a particular species


```r
perturb &lt;- read.csv("./Data/perturb_predictions.csv")
perturb
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Case.Number setosa
## 1     5.314807     4.04260     5.856493   1.5136405           1  0.000
## 2     4.546596     2.73312     1.028409   0.4986859           1  1.000
## 3     5.467831     2.98690     6.565054   0.2349578           1  0.468
## 4     6.935536     2.38896     5.548015   0.5669754           1  0.468
##   versicolor virginica
## 1      0.234     0.766
## 2      0.000     0.000
## 3      0.116     0.416
## 4      0.082     0.450
```

---

## Step 5: Obtain similarity score between data observation and sampled values

We need to determine how similar a sampled case is to the observed case in the testing data

Case 1 from testing data:

```r
iris_test[1, ]
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          5.1         3.5          1.4         0.2
```

First sample from training data variable distributions associated with case 1 of testing data:

```r
perturb[1, 1:4]
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     5.314807      4.0426     5.856493    1.513641
```

---- 

LIME uses exponential kernel function
  `$$\pi_{x_{obs}}(x_{sampled}) = exp\left\{\frac{−D(x_{obs}, \ x_{sampled})^2}{σ^2}\right\}$$`
where

`\(x_{obs}\)`: observed data vector to predict  

`\(x_{sampled}\)`: sampled data vector from distribution of training variables  

`\(D(\cdot \ , \ \cdot)\)`: distance function such as euclidean distance, cosine distance, etc.  
`\(\sigma\)`: width (default set to 0.75 in `lime`)
 
---
 
## Step 6: Perform feature selection by fiting a model to the sampled data and associated predictions (weighted by the similarity scores)

* The user can specify the number of variables (features) they would like to select: `\(m\)`
* With the iris data, the following three models will be fit to perform variable selection to select `\(m=2\)` features:

`$$\mbox{P(setosa)} \sim \mbox{Sepal.Length} + \mbox{Sepal.Width} + \mbox{Petal.Length} + \mbox{Petal.Width}$$`
`$$\mbox{P(versicolor)} \sim \mbox{Sepal.Length} + \mbox{Sepal.Width} + \mbox{Petal.Length} + \mbox{Petal.Width}$$`
`$$\mbox{P(virginica)} \sim \mbox{Sepal.Length} + \mbox{Sepal.Width} + \mbox{Petal.Length} + \mbox{Petal.Width}$$`

----

* To perform variable selection `lime` supports: 
    - forward selection with ridge regression
    - highest weight with ridge regression
    - LASSO
    - tree models
    - auto: forward selection if `\(m\le6\)`, highest weight otherwise

---

## Step 7: Fit a simple model to regress the predictions on the `\(m\)` selected predictor variables (weighted by the similarity scores)

* Currently, `lime` is programmed to use ridge regression as the "simple" model
* If the response is categorical, the user can select how many categories they want to explain
* In this example, only setosa will be explained
* If petal length and sepal length were selected as the most important features for the first case in the testing data, then the simple model is

  $$\mbox{P(Setosa)} \sim \mbox{Petal.Length} + \mbox{Sepal.Length} $$

---

## Step 8: Extract the feature weights and use them as the explanations

* Steps 3 through 8 are all performed using the `explain` function in `lime`
* Steps 3 through 7 all happen behind the scenes
* The "feature weights" that are extracted are the coefficients for the predictor variables from the ridge regression


```r
# Explain new observation
explanation &lt;- explain(iris_test, explainer, n_labels = 1, 
                       n_features = 2, n_permutations = 5000,
                       feature_select = 'auto')
```

----


```r
explanation[1:2, 1:6]
```

```
##       model_type case  label label_prob  model_r2 model_intercept
## 1 classification    1 setosa          1 0.3944587       0.2470557
## 2 classification    1 setosa          1 0.3944587       0.2470557
```

```r
explanation[1:2, 7:10]
```

```
##   model_prediction      feature feature_value feature_weight
## 1        0.6969047 Sepal.Length           5.1   0.0003415683
## 2        0.6969047  Petal.Width           0.2   0.4495073822
```

```r
explanation[1:2, 11:13]
```

```
##          feature_desc               data prediction
## 1 Sepal.Length &lt;= 5.2 5.1, 3.5, 1.4, 0.2    1, 0, 0
## 2  Petal.Width &lt;= 0.4 5.1, 3.5, 1.4, 0.2    1, 0, 0
```

---

## | Plot of explanations for predictions from random forest model trained on iris data


```r
plot_features(explanation)
```

![](An_Application_of_LIME_to_a_Random_Forest_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---
class: inverse, center, middle

# Issues with LIME

---
class: inverse, center, middle

# Attempts at a Solution

---
class: inverse, center, middle

# Future Work
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
